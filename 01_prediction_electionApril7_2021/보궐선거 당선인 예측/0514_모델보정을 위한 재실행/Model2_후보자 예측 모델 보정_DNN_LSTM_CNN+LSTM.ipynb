{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델2\n",
    "- 2021.05.14 : 전처리( 문장분리, 맞춤법, 띄어쓰기 교정 )을 하지 않은 데이터로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:11.571157Z",
     "start_time": "2021-05-13T16:21:08.337803Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno\n",
    "import re\n",
    "import os\n",
    "# 데이터\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 경고메시지 제거\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# KFold\n",
    "from sklearn.model_selection import KFold # 순서대로 하거나 or 랜덤하게 클래스를 나눈다.\n",
    "from sklearn.model_selection import StratifiedKFold # 결과데이터를 보고 각 클래스가 균등한 비율로 들어있게끔 나눈다.\n",
    "\n",
    "# 교차검증 함수\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 학습데이터와 검증데이터로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 전처리\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    \n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# 추가항목\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 비지도학습 - 군집\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import mean_shift\n",
    "\n",
    "# 딥러닝\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "# CNN\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, Conv1D\n",
    "from keras.layers import MaxPooling2D, MaxPooling1D\n",
    "\n",
    "# NLP\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# 문장을 단어 단위로 나누기\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# 다중분류를 위한 원-핫 인코더\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 더이상 성능 향상이 이루어지지 않는다면 조기 중단시킬 수 있는 함수\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# epoch마다 모델을 저장하는 함수\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 파일로 저장된 딥러닝 모델을 객체로 복구하는 함수\n",
    "from keras.models import load_model\n",
    "\n",
    "# 저장\n",
    "import pickle\n",
    "\n",
    "# 시간 모듈\n",
    "import time # 현재 시간값을 구할 수 있다.\n",
    "\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 13\n",
    "plt.rcParams['figure.figsize'] = 10,5\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:12.397948Z",
     "start_time": "2021-05-13T16:21:11.573154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# GPU 메모리 사용량을 필요한 만큼만 증가하도록 설정\n",
    "\n",
    "# 사용가능한 GPU 목록 가져오기\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        # 필요한 만큼만 메모리를 사용할 수 있도록 설정하기\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:12.415899Z",
     "start_time": "2021-05-13T16:21:12.398945Z"
    }
   },
   "outputs": [],
   "source": [
    "# RandomSeed\n",
    "seed= 3\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:13.765291Z",
     "start_time": "2021-05-13T16:21:12.416897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144139, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>날짜</th>\n",
       "      <th>작성일</th>\n",
       "      <th>댓글</th>\n",
       "      <th>출처</th>\n",
       "      <th>지역(서울:1, 부산:2)</th>\n",
       "      <th>정당(1:더불어민주당,2:국민의힘)</th>\n",
       "      <th>정당평가(부정;0, 긍정:1)</th>\n",
       "      <th>후보(기호 순)</th>\n",
       "      <th>후보평가(부정;0, 긍정:1)</th>\n",
       "      <th>제목댓글</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-12 19:13:00</td>\n",
       "      <td>철수야! 뜸 들이지 말고 애국하는 마음으로 물러서라~~~</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 철수야! 뜸 들이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-09 13:49:00</td>\n",
       "      <td>박영선은 정동영이 얻은 36프로선에 머무를것. 4.7.이후 OOO정권은 몰락의 길 ...</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 박영선은 정동영이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-07 14:42:00</td>\n",
       "      <td>빵선이가서울시장되면서울은공산국가수도제2의평양이될것이다</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 빵선이가서울시장되...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-07 14:38:00</td>\n",
       "      <td>서울시장후보더듬당박빵선이는절대로서울시장을할수없다이유는가족은미국.영국에 영주권자이므로...</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 서울시장후보더듬당...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-07 14:02:00</td>\n",
       "      <td>부산은오거돈선거이고 오거돈치부선거아닌가 오거돈에 성추해으로 생긴선거가 가독도신공항은...</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 부산은오거돈선거이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     제목                   날짜  \\\n",
       "0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "1  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "2  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "3  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "4  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "\n",
       "                   작성일                                                 댓글  \\\n",
       "0  2021-03-12 19:13:00                    철수야! 뜸 들이지 말고 애국하는 마음으로 물러서라~~~   \n",
       "1  2021-03-09 13:49:00  박영선은 정동영이 얻은 36프로선에 머무를것. 4.7.이후 OOO정권은 몰락의 길 ...   \n",
       "2  2021-03-07 14:42:00                      빵선이가서울시장되면서울은공산국가수도제2의평양이될것이다   \n",
       "3  2021-03-07 14:38:00  서울시장후보더듬당박빵선이는절대로서울시장을할수없다이유는가족은미국.영국에 영주권자이므로...   \n",
       "4  2021-03-07 14:02:00  부산은오거돈선거이고 오거돈치부선거아닌가 오거돈에 성추해으로 생긴선거가 가독도신공항은...   \n",
       "\n",
       "     출처  지역(서울:1, 부산:2)  정당(1:더불어민주당,2:국민의힘)  정당평가(부정;0, 긍정:1)  후보(기호 순)  \\\n",
       "0  조선일보             NaN                  NaN               NaN       NaN   \n",
       "1  조선일보             1.0                  NaN               NaN       1.0   \n",
       "2  조선일보             1.0                  NaN               NaN       1.0   \n",
       "3  조선일보             1.0                  1.0               0.0       1.0   \n",
       "4  조선일보             2.0                  1.0               0.0       NaN   \n",
       "\n",
       "   후보평가(부정;0, 긍정:1)                                               제목댓글  \n",
       "0               NaN  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 철수야! 뜸 들이...  \n",
       "1               0.0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 박영선은 정동영이...  \n",
       "2               0.0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 빵선이가서울시장되...  \n",
       "3               0.0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 서울시장후보더듬당...  \n",
       "4               NaN  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 부산은오거돈선거이...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/재보궐선거댓글데이터_최종_유튜브수정_0429.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "- 후보자별 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:13.816155Z",
     "start_time": "2021-05-13T16:21:13.766289Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사용할 데이터\n",
    "df2 = df[['지역(서울:1, 부산:2)', '후보(기호 순)', '후보평가(부정;0, 긍정:1)',\n",
    "          '정당(1:더불어민주당,2:국민의힘)', '정당평가(부정;0, 긍정:1)', '제목댓글']]\n",
    "df2.columns = ['area', 'candidate', 'candidate_eval',\n",
    "               'party', 'party_eval', 'title_comment']\n",
    "\n",
    "# 후보자를 구별하여 'area_candidate' 컬럼에 각 후보자 이름 추가\n",
    "\n",
    "# 후보자별 인덱스 추출\n",
    "ys_idx = df2.query('area == 1.0 & candidate == 1.0').index  # 박영선\n",
    "sh_idx = df2.query('area == 1.0 & candidate == 2.0').index  # 오세훈\n",
    "yc_idx = df2.query('area == 2.0 & candidate == 1.0').index  # 김영춘\n",
    "hj_idx = df2.query('area == 2.0 & candidate == 2.0').index  # 박형준\n",
    "etc_idx = df2.query('candidate == 5.0').index  # 기타\n",
    "\n",
    "# 'area_candidate'컬럼에 후보자 이름값 추가\n",
    "df2['area_candidate'] = np.nan\n",
    "df2['area_candidate'][ys_idx] = '박영선'\n",
    "df2['area_candidate'][sh_idx] = '오세훈'\n",
    "df2['area_candidate'][yc_idx] = '김영춘'\n",
    "df2['area_candidate'][hj_idx] = '박형준'\n",
    "df2['area_candidate'][etc_idx] = '기타'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:13.839125Z",
     "start_time": "2021-05-13T16:21:13.817153Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 라벨링된 데이터수: 4332\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>기타</th>\n",
       "      <td>1611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>박영선</th>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>오세훈</th>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>박형준</th>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>김영춘</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     area_candidate\n",
       "기타             1611\n",
       "박영선            1303\n",
       "오세훈            1178\n",
       "박형준             211\n",
       "김영춘              29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 종류별 개수 확인\n",
    "print(f'총 라벨링된 데이터수: {df2[\"area_candidate\"].value_counts().sum()}')\n",
    "pd.DataFrame(df2['area_candidate'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 다중분류\n",
    "- **활성화 함수**를 **이용**하려면 Y값이 **0-1로 이루어져** 있어야 함\n",
    "    - 원-핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 학습할 텍스트(docs) 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:13.844086Z",
     "start_time": "2021-05-13T16:21:13.841090Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한글 정규표현식\n",
    "def text_cleaning(text) :\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    result = hangul.sub('', str(text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:14.599063Z",
     "start_time": "2021-05-13T16:21:13.846076Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한글 외 문자열 공백처리\n",
    "df2['title_comment'] = df2['title_comment'].apply(lambda x: text_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:14.627986Z",
     "start_time": "2021-05-13T16:21:14.600061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4332"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 후보자를 5가지로 분리가능한 데이터수\n",
    "notnull_idx = df2[ df2['area_candidate'].notnull() ].index\n",
    "\n",
    "# 후보자 예측에 사용될 feature 텍스트 선정\n",
    "docs = df2['title_comment'][notnull_idx].to_list()\n",
    "len(docs) # 총 라벨링된 데이터수랑 같아야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  y값 문자열 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:14.637957Z",
     "start_time": "2021-05-13T16:21:14.629982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 4, 0, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열 인코딩\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# 후보값을 LabelEncoding\n",
    "y = encoder.fit_transform(df2['area_candidate'][notnull_idx].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:14.646934Z",
     "start_time": "2021-05-13T16:21:14.638955Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기타' '김영춘' '박영선' '박형준' '오세훈']\n",
      "['기타']\n",
      "['김영춘']\n",
      "['박영선']\n",
      "['박형준']\n",
      "['오세훈']\n"
     ]
    }
   ],
   "source": [
    "# 인코딩값 확인\n",
    "print( encoder.classes_ )\n",
    "print( encoder.inverse_transform([0]) )\n",
    "print( encoder.inverse_transform([1]) )\n",
    "print( encoder.inverse_transform([2]) )\n",
    "print( encoder.inverse_transform([3]) )\n",
    "print( encoder.inverse_transform([4]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  y값 One-Hot Encoding\n",
    "- 출력층 node의 개수는 **5개**로 맞춰주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:14.663889Z",
     "start_time": "2021-05-13T16:21:14.648931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4332, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded = to_categorical(y)\n",
    "y_encoded.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:14.681840Z",
     "start_time": "2021-05-13T16:21:14.665884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded # ⭐출력층 노드의 개수는 5로 맞춰주기⭐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Tokenizer\n",
    "- 텍스트를 잘게 나누는 것\n",
    "- 단어별, 문장별, 형태소별, ...\n",
    "- 이렇게 나누어진 하나의 단위를 **Token(토큰)**\n",
    "\n",
    "\n",
    "- **빈도에 따라 번호가 정해지도록 만들기**\n",
    "    - Tokenizer(num_words=5000): 빈도가 높은 토큰들 중 5000개 토큰만 선택해서 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:03.696763Z",
     "start_time": "2021-05-13T16:37:03.315099Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'서울시장': 1,\n",
       " '박영선': 2,\n",
       " '오세훈': 3,\n",
       " '보궐선거': 4,\n",
       " '내곡동': 5,\n",
       " '땅': 6,\n",
       " '후보': 7,\n",
       " '안철수': 8,\n",
       " '주영진의': 9,\n",
       " '토론': 10,\n",
       " '후보자': 11,\n",
       " '선거': 12,\n",
       " '원인첫': 13,\n",
       " '어땠나': 14,\n",
       " '박형준': 15,\n",
       " '마지막': 16,\n",
       " '민주당': 17,\n",
       " '윤석열': 18,\n",
       " '당신의': 19,\n",
       " '회': 20,\n",
       " '분토론': 21,\n",
       " '선택은': 22,\n",
       " '년': 23,\n",
       " '왜': 24,\n",
       " '서울': 25,\n",
       " '이': 26,\n",
       " '나경원': 27,\n",
       " '그': 28,\n",
       " '다': 29,\n",
       " '대선': 30,\n",
       " '또': 31,\n",
       " '하는': 32,\n",
       " '수': 33,\n",
       " '일': 34,\n",
       " '더': 35,\n",
       " '논란': 36,\n",
       " '것': 37,\n",
       " '부산시장': 38,\n",
       " '안': 39,\n",
       " '박원순': 40,\n",
       " '지지율': 41,\n",
       " '무슨': 42,\n",
       " '없다': 43,\n",
       " '토론회': 44,\n",
       " '할': 45,\n",
       " '대통령': 46,\n",
       " '성추행': 47,\n",
       " '여론조사와': 48,\n",
       " '오세훈이': 49,\n",
       " '등': 50,\n",
       " '한': 51,\n",
       " '측량때': 52,\n",
       " '다른': 53,\n",
       " '사람': 54,\n",
       " '보면': 55,\n",
       " '강경': 56,\n",
       " '여론조사박영선': 57,\n",
       " '있었냐가': 58,\n",
       " '윤석열의': 59,\n",
       " '몰랐다던': 60,\n",
       " '오세훈땅': 61,\n",
       " '잘': 62,\n",
       " '단일화': 63,\n",
       " '되면': 64,\n",
       " '있는': 65,\n",
       " '없는': 66,\n",
       " '이수봉': 67,\n",
       " '이런': 68,\n",
       " '다시보기': 69,\n",
       " '참': 70,\n",
       " '달리': 71,\n",
       " '너무': 72,\n",
       " '민심은': 73,\n",
       " '듯': 74,\n",
       " '앞서': 75,\n",
       " '더불어민주당': 76,\n",
       " '거짓말': 77,\n",
       " '인방': 78,\n",
       " '바닥': 79,\n",
       " '같은': 80,\n",
       " '합당': 81,\n",
       " '김진애': 82,\n",
       " '부산': 83,\n",
       " '분노': 84,\n",
       " '대통령이': 85,\n",
       " '고민정': 86,\n",
       " '부동산': 87,\n",
       " '된': 88,\n",
       " '문': 89,\n",
       " '앞두고': 90,\n",
       " '도대체': 91,\n",
       " '시장': 92,\n",
       " '좀': 93,\n",
       " '전': 94,\n",
       " '눈': 95,\n",
       " '하고': 96,\n",
       " '태풍의': 97,\n",
       " '한다': 98,\n",
       " '쫓아내야': 99,\n",
       " '진정한': 100,\n",
       " '사과에': 101,\n",
       " '박영선은': 102,\n",
       " '출마': 103,\n",
       " '파괴력은': 104,\n",
       " '어떤': 105,\n",
       " '후': 106,\n",
       " '레이스': 107,\n",
       " '김영춘': 108,\n",
       " '사퇴': 109,\n",
       " '야당': 110,\n",
       " '거의': 111,\n",
       " '일이': 112,\n",
       " '의혹': 113,\n",
       " '오세훈의': 114,\n",
       " '받으면': 115,\n",
       " '그리고': 116,\n",
       " '월': 117,\n",
       " '주호영': 118,\n",
       " '연속': 119,\n",
       " '메시지문': 120,\n",
       " '피해자': 121,\n",
       " '죄': 122,\n",
       " '못': 123,\n",
       " '것을': 124,\n",
       " '낮게': 125,\n",
       " '문재인': 126,\n",
       " '올린': 127,\n",
       " '갚겠다': 128,\n",
       " '바람': 129,\n",
       " '국민의힘과': 130,\n",
       " '위': 131,\n",
       " '사과': 132,\n",
       " '년만에': 133,\n",
       " '합니다': 134,\n",
       " '화이팅': 135,\n",
       " '정말': 136,\n",
       " '그냥': 137,\n",
       " '선출': 138,\n",
       " '재도전': 139,\n",
       " '대세론': 140,\n",
       " '꺾은': 141,\n",
       " '오세훈임기': 142,\n",
       " '못마친': 143,\n",
       " '울먹': 144,\n",
       " '그렇게': 145,\n",
       " '그런': 146,\n",
       " '국민의': 147,\n",
       " '번': 148,\n",
       " '전세금': 149,\n",
       " '임대인': 150,\n",
       " '사람이': 151,\n",
       " '누가': 152,\n",
       " '어떻게': 153,\n",
       " '발언': 154,\n",
       " '지금': 155,\n",
       " '늦지': 156,\n",
       " '것이다': 157,\n",
       " '스모킹건': 158,\n",
       " '생태탕의혹': 159,\n",
       " '총정리': 160,\n",
       " '가덕도': 161,\n",
       " '거': 162,\n",
       " '한방에': 163,\n",
       " '것이': 164,\n",
       " '있다': 165,\n",
       " 'ㅋㅋ': 166,\n",
       " '휘청': 167,\n",
       " '새로운': 168,\n",
       " '하면': 169,\n",
       " '이낙연': 170,\n",
       " '박영선이': 171,\n",
       " '내': 172,\n",
       " '메시지': 173,\n",
       " '속': 174,\n",
       " '마음': 175,\n",
       " '뉴스브리핑박영선': 176,\n",
       " '때': 177,\n",
       " '진짜': 178,\n",
       " '측': 179,\n",
       " '얼마나': 180,\n",
       " '이제': 181,\n",
       " '올라': 182,\n",
       " '큰': 183,\n",
       " '이해찬의': 184,\n",
       " '많이': 185,\n",
       " '뭘': 186,\n",
       " '당은': 187,\n",
       " '참모': 188,\n",
       " '이겼다는': 189,\n",
       " '효과는': 190,\n",
       " '차마': 191,\n",
       " '정치검사의': 192,\n",
       " '퇴장': 193,\n",
       " '보강된': 194,\n",
       " '꼭': 195,\n",
       " '다시': 196,\n",
       " '난': 197,\n",
       " '대의': 198,\n",
       " '정권심판론': 199,\n",
       " '시민이': 200,\n",
       " 'ㅋㅋㅋ': 201,\n",
       " '편의점': 202,\n",
       " '복잡한': 203,\n",
       " '될': 204,\n",
       " '정치': 205,\n",
       " '아파트': 206,\n",
       " '찍겠고': 207,\n",
       " '일은': 208,\n",
       " '논평': 209,\n",
       " '말고': 210,\n",
       " '주자': 211,\n",
       " '위해': 212,\n",
       " '리더십': 213,\n",
       " '때문에': 214,\n",
       " '소리': 215,\n",
       " '몰라': 216,\n",
       " '선언': 217,\n",
       " '만들겠다': 218,\n",
       " '알바': 219,\n",
       " '무인': 220,\n",
       " '승리하는': 221,\n",
       " '어찌': 222,\n",
       " '저': 223,\n",
       " '아니라': 224,\n",
       " '날입니다': 225,\n",
       " '국민': 226,\n",
       " '제대로': 227,\n",
       " '민주': 228,\n",
       " '현실': 229,\n",
       " '청년': 230,\n",
       " '이렇게': 231,\n",
       " '것은': 232,\n",
       " '아니냐': 233,\n",
       " '정부': 234,\n",
       " '대한': 235,\n",
       " '협상': 236,\n",
       " '뉴스브리핑오세훈': 237,\n",
       " '우세': 238,\n",
       " '후보를': 239,\n",
       " '이언주': 240,\n",
       " '오세훈은': 241,\n",
       " '나라를': 242,\n",
       " '보수': 243,\n",
       " '고충에': 244,\n",
       " '악의적': 245,\n",
       " '국민의힘': 246,\n",
       " '아닌가': 247,\n",
       " '정치를': 248,\n",
       " '거짓말을': 249,\n",
       " 'ㅋ': 250,\n",
       " '여론조사': 251,\n",
       " '국민을': 252,\n",
       " '투기': 253,\n",
       " '나는': 254,\n",
       " '시민참여유세': 255,\n",
       " '사업가의': 256,\n",
       " '처절한': 257,\n",
       " '국민이': 258,\n",
       " '게': 259,\n",
       " '년의': 260,\n",
       " '아닌': 261,\n",
       " '차': 262,\n",
       " '후보님': 263,\n",
       " '도쿄': 264,\n",
       " '이유': 265,\n",
       " '엘시티': 266,\n",
       " '버렸다': 267,\n",
       " '말이': 268,\n",
       " '보고': 269,\n",
       " '해': 270,\n",
       " '이러니': 271,\n",
       " '이미지': 272,\n",
       " '어용': 273,\n",
       " '두려워할': 274,\n",
       " '후보가': 275,\n",
       " '대한민국': 276,\n",
       " '후보에': 277,\n",
       " '만드는': 278,\n",
       " '기억': 279,\n",
       " '라떼레전드': 280,\n",
       " '향한': 281,\n",
       " '집념의원직마저': 282,\n",
       " '모두': 283,\n",
       " '정권': 284,\n",
       " '아니다': 285,\n",
       " '보궐': 286,\n",
       " '계단': 287,\n",
       " '잡은': 288,\n",
       " '계속': 289,\n",
       " '오세훈안철수': 290,\n",
       " '자기': 291,\n",
       " '위하여': 292,\n",
       " '듣는': 293,\n",
       " '행사에': 294,\n",
       " '의전용': 295,\n",
       " '바로': 296,\n",
       " '그래도': 297,\n",
       " '없고': 298,\n",
       " '확정': 299,\n",
       " '해도': 300,\n",
       " '토론회박영선': 301,\n",
       " '격차': 302,\n",
       " '안철수홍준표유승민신기루': 303,\n",
       " '방송이라는': 304,\n",
       " '식목': 305,\n",
       " '반드시': 306,\n",
       " '신공항': 307,\n",
       " '민주당은': 308,\n",
       " '되는': 309,\n",
       " '난타전내곡동': 310,\n",
       " '몹쓸짓': 311,\n",
       " '만원짜리': 312,\n",
       " '함께': 313,\n",
       " '알고': 314,\n",
       " '응원합니다': 315,\n",
       " '제발': 316,\n",
       " '이번': 317,\n",
       " '뭐': 318,\n",
       " '절대': 319,\n",
       " '민주당이': 320,\n",
       " '모든': 321,\n",
       " '가지고': 322,\n",
       " '가': 323,\n",
       " '국민들의': 324,\n",
       " '한겨레': 325,\n",
       " '개': 326,\n",
       " '선거를': 327,\n",
       " '나라가': 328,\n",
       " '해야': 329,\n",
       " '나라': 330,\n",
       " '해라': 331,\n",
       " '아무리': 332,\n",
       " '여론': 333,\n",
       " '흘려': 334,\n",
       " '발목': 335,\n",
       " '점점': 336,\n",
       " '아니고': 337,\n",
       " '아직도': 338,\n",
       " '땀': 339,\n",
       " '봉사한': 340,\n",
       " '특혜라고': 341,\n",
       " '하십니까': 342,\n",
       " '오세훈박영선': 343,\n",
       " '나경원이언주': 344,\n",
       " '리얼미터': 345,\n",
       " '여론조사오세훈': 346,\n",
       " '해서': 347,\n",
       " '하는데': 348,\n",
       " '하지': 349,\n",
       " '있나': 350,\n",
       " '뭐가': 351,\n",
       " '내가': 352,\n",
       " '걸': 353,\n",
       " '앞에': 354,\n",
       " '야권': 355,\n",
       " '기호': 356,\n",
       " '승리를': 357,\n",
       " 'ㅉㅉ': 358,\n",
       " '줄': 359,\n",
       " '그러나': 360,\n",
       " '아니': 361,\n",
       " '만들어': 362,\n",
       " '정신': 363,\n",
       " '지': 364,\n",
       " '순간': 365,\n",
       " '아주': 366,\n",
       " '이미': 367,\n",
       " '갔다': 368,\n",
       " '치고': 369,\n",
       " '억': 370,\n",
       " '오오오오세훈': 371,\n",
       " '민주당에': 372,\n",
       " '갤럽': 373,\n",
       " '여파': 374,\n",
       " '취임': 375,\n",
       " '최저치': 376,\n",
       " '항상': 377,\n",
       " '않는': 378,\n",
       " '밖에': 379,\n",
       " '없이': 380,\n",
       " '건': 381,\n",
       " '가장': 382,\n",
       " '있고': 383,\n",
       " '이명박': 384,\n",
       " '내곡동땅': 385,\n",
       " '허리를': 386,\n",
       " '휘게': 387,\n",
       " '증세': 388,\n",
       " '한명숙': 389,\n",
       " '서울시': 390,\n",
       " '저런': 391,\n",
       " '없어': 392,\n",
       " '많은': 393,\n",
       " '그만': 394,\n",
       " '대한민국의': 395,\n",
       " '국회의원': 396,\n",
       " '돈': 397,\n",
       " '같다': 398,\n",
       " '대': 399,\n",
       " '후보로': 400,\n",
       " '일본': 401,\n",
       " '결국': 402,\n",
       " '물타기': 403,\n",
       " '완전': 404,\n",
       " '대로': 405,\n",
       " '수사팀': 406,\n",
       " '사람은': 407,\n",
       " '의원직': 408,\n",
       " '니가': 409,\n",
       " '박영선의': 410,\n",
       " '사전투표': 411,\n",
       " '말': 412,\n",
       " '침대축구냐': 413,\n",
       " '초조하냐': 414,\n",
       " '거칠어진다': 415,\n",
       " '이제는': 416,\n",
       " '당선': 417,\n",
       " '잘못': 418,\n",
       " '한다고': 419,\n",
       " '핵심은': 420,\n",
       " '박영선오세훈': 421,\n",
       " '측량': 422,\n",
       " '차례': 423,\n",
       " '아들에게서': 424,\n",
       " '샀다': 425,\n",
       " '반박남는': 426,\n",
       " '문재인이': 427,\n",
       " '제': 428,\n",
       " '제일': 429,\n",
       " '그래서': 430,\n",
       " '안되는': 431,\n",
       " '역시': 432,\n",
       " '영선아': 433,\n",
       " '만에': 434,\n",
       " '너': 435,\n",
       " '판세는': 436,\n",
       " '뉴있저': 437,\n",
       " '내곡': 438,\n",
       " '대전보궐선거': 439,\n",
       " '판세': 440,\n",
       " '영향은': 441,\n",
       " '배수진': 442,\n",
       " '걷어차고': 443,\n",
       " '국민들이': 444,\n",
       " '자신의': 445,\n",
       " '그게': 446,\n",
       " '더러운': 447,\n",
       " '나': 448,\n",
       " '말을': 449,\n",
       " '같이': 450,\n",
       " '보다': 451,\n",
       " '하나': 452,\n",
       " '보다는': 453,\n",
       " '전직': 454,\n",
       " '우리': 455,\n",
       " '지지율은': 456,\n",
       " '뉴스브리핑영선아': 457,\n",
       " '주': 458,\n",
       " '윤석열은': 459,\n",
       " '선거처럼달라진': 460,\n",
       " '반토막': 461,\n",
       " '성추행은': 462,\n",
       " '압박': 463,\n",
       " '중립성': 464,\n",
       " '시장이': 465,\n",
       " '만든': 466,\n",
       " '바란다': 467,\n",
       " '사람들': 468,\n",
       " '선거에': 469,\n",
       " '딱': 470,\n",
       " '는': 471,\n",
       " '한번': 472,\n",
       " '전부': 473,\n",
       " '오': 474,\n",
       " '윤석열이': 475,\n",
       " '뭔': 476,\n",
       " '영선이': 477,\n",
       " '논썰': 478,\n",
       " '직행': 479,\n",
       " '지난': 480,\n",
       " '못하는': 481,\n",
       " '않고': 482,\n",
       " '보는': 483,\n",
       " '쓰레기': 484,\n",
       " '있으면': 485,\n",
       " '하면서': 486,\n",
       " '아는': 487,\n",
       " '놈이': 488,\n",
       " '전혀': 489,\n",
       " '박': 490,\n",
       " '조사': 491,\n",
       " '인간이': 492,\n",
       " '홍준표': 493,\n",
       " '생각이': 494,\n",
       " '원팀': 495,\n",
       " '토론회오세훈': 496,\n",
       " '사기꾼': 497,\n",
       " '겸손박영선': 498,\n",
       " '자초한': 499,\n",
       " '불충분한': 500,\n",
       " '찾아': 501,\n",
       " '대검': 502,\n",
       " '있습니다': 503,\n",
       " '정권을': 504,\n",
       " '이게': 505,\n",
       " '못할': 506,\n",
       " '같은데': 507,\n",
       " '무조건': 508,\n",
       " '시장으로': 509,\n",
       " '여자': 510,\n",
       " '표': 511,\n",
       " '모르는': 512,\n",
       " '못한': 513,\n",
       " '도': 514,\n",
       " '이거': 515,\n",
       " '있다고': 516,\n",
       " '이젠': 517,\n",
       " '시장을': 518,\n",
       " '백신': 519,\n",
       " '인간': 520,\n",
       " '니': 521,\n",
       " '재난지원금': 522,\n",
       " '사람을': 523,\n",
       " '승': 524,\n",
       " '반대': 525,\n",
       " '두둔': 526,\n",
       " '강조한': 527,\n",
       " '이재명윤석열': 528,\n",
       " '향해선': 529,\n",
       " '등에': 530,\n",
       " '무혐의법무부': 531,\n",
       " '합동감찰로': 532,\n",
       " '수사관행': 533,\n",
       " '개선할': 534,\n",
       " '대결선': 535,\n",
       " '정치적': 536,\n",
       " '후보는': 537,\n",
       " '그럼': 538,\n",
       " '열심히': 539,\n",
       " '모르고': 540,\n",
       " '되지': 541,\n",
       " '어느': 542,\n",
       " '자체가': 543,\n",
       " '대가': 544,\n",
       " '총선': 545,\n",
       " '앵커': 546,\n",
       " '입당하라': 547,\n",
       " '안들어오면': 548,\n",
       " '국회': 549,\n",
       " '초청박영선': 550,\n",
       " '족적': 551,\n",
       " '눈부셔': 552,\n",
       " '실수흠결로': 553,\n",
       " '축소': 554,\n",
       " '서울은': 555,\n",
       " '아직': 556,\n",
       " '먼저': 557,\n",
       " '부터': 558,\n",
       " '더불어': 559,\n",
       " '나쁜': 560,\n",
       " '세금': 561,\n",
       " '사건': 562,\n",
       " '받고': 563,\n",
       " '나와서': 564,\n",
       " '이야기': 565,\n",
       " '수도': 566,\n",
       " '반값': 567,\n",
       " 'ㅋㅋㅋㅋ': 568,\n",
       " '바랍니다': 569,\n",
       " '것도': 570,\n",
       " '와': 571,\n",
       " '처가': 572,\n",
       " '분노를': 573,\n",
       " '단일후보': 574,\n",
       " '정의는': 575,\n",
       " '재보궐': 576,\n",
       " '여러분': 577,\n",
       " '검찰': 578,\n",
       " '짓': 579,\n",
       " '말도': 580,\n",
       " '어디': 581,\n",
       " '서로': 582,\n",
       " '말은': 583,\n",
       " '위한': 584,\n",
       " '된다': 585,\n",
       " '뻔뻔': 586,\n",
       " '아니면': 587,\n",
       " '아': 588,\n",
       " '못하고': 589,\n",
       " '조국': 590,\n",
       " '있을까': 591,\n",
       " '생각합니다': 592,\n",
       " '해놓고': 593,\n",
       " '서울을': 594,\n",
       " '좋은': 595,\n",
       " '오늘': 596,\n",
       " '조강지처': 597,\n",
       " '후보의': 598,\n",
       " '임기': 599,\n",
       " '칼럼': 600,\n",
       " '오후에도': 601,\n",
       " '결렬각자': 602,\n",
       " '양자': 603,\n",
       " '만': 604,\n",
       " '사설': 605,\n",
       " '박영선을': 606,\n",
       " '안된다': 607,\n",
       " '시민들': 608,\n",
       " '있을': 609,\n",
       " '보니': 610,\n",
       " '있는데': 611,\n",
       " '하지만': 612,\n",
       " '를': 613,\n",
       " '땅이': 614,\n",
       " '있지': 615,\n",
       " '자가': 616,\n",
       " '부산에서': 617,\n",
       " '두': 618,\n",
       " '타고': 619,\n",
       " '국민들': 620,\n",
       " '사람들이': 621,\n",
       " '인물이': 622,\n",
       " '보인다': 623,\n",
       " '변절자': 624,\n",
       " '잘못된': 625,\n",
       " '무상급식': 626,\n",
       " '파이팅': 627,\n",
       " '두번째': 628,\n",
       " '수준이': 629,\n",
       " '정책': 630,\n",
       " '해제': 631,\n",
       " '많아': 632,\n",
       " '박원순이': 633,\n",
       " '박형준이': 634,\n",
       " '하겠다고': 635,\n",
       " '안철수는': 636,\n",
       " '요즘': 637,\n",
       " '대해': 638,\n",
       " '여성': 639,\n",
       " '표를': 640,\n",
       " '넌': 641,\n",
       " '한심한': 642,\n",
       " '이건': 643,\n",
       " '이상': 644,\n",
       " '그리': 645,\n",
       " '일본에': 646,\n",
       " '박영선정부때까지': 647,\n",
       " '전정부': 648,\n",
       " '탓': 649,\n",
       " '국민은': 650,\n",
       " '실현': 651,\n",
       " '자꾸': 652,\n",
       " '승리': 653,\n",
       " '반성하며': 654,\n",
       " '출마선언': 655,\n",
       " '영입한': 656,\n",
       " '김문수에': 657,\n",
       " '질문했다': 658,\n",
       " '잘려': 659,\n",
       " '에': 660,\n",
       " '못하게': 661,\n",
       " '행위': 662,\n",
       " '심의': 663,\n",
       " '부정': 664,\n",
       " '땅투기': 665,\n",
       " '첫': 666,\n",
       " '정권이': 667,\n",
       " '추진': 668,\n",
       " '맞은': 669,\n",
       " '대통령국민의힘': 670,\n",
       " '쇼로': 671,\n",
       " '불안감': 672,\n",
       " '불식': 673,\n",
       " '마친': 674,\n",
       " '부족함': 675,\n",
       " '하락후회': 676,\n",
       " '중랑물재생센터': 677,\n",
       " '반값아파트그린벨트': 678,\n",
       " '변신의': 679,\n",
       " '귀재': 680,\n",
       " '놓고': 681,\n",
       " '것입니다': 682,\n",
       " '장관': 683,\n",
       " '날': 684,\n",
       " '입니다': 685,\n",
       " '힘을': 686,\n",
       " '약발': 687,\n",
       " '안먹히나부산시장': 688,\n",
       " '독주': 689,\n",
       " '김종인': 690,\n",
       " '피해를': 691,\n",
       " '빨리': 692,\n",
       " '국민들은': 693,\n",
       " '물고': 694,\n",
       " '차라리': 695,\n",
       " '오히려': 696,\n",
       " '권력을': 697,\n",
       " '문제가': 698,\n",
       " '있네': 699,\n",
       " '되어': 700,\n",
       " '현재': 701,\n",
       " '앞으로': 702,\n",
       " '만원': 703,\n",
       " '진실을': 704,\n",
       " '받은': 705,\n",
       " '중': 706,\n",
       " '도전': 707,\n",
       " '아예': 708,\n",
       " '숨은': 709,\n",
       " '공약': 710,\n",
       " '버렸다에': 711,\n",
       " '묵과': 712,\n",
       " '범법': 713,\n",
       " '인터뷰': 714,\n",
       " '태극기': 715,\n",
       " '시장님': 716,\n",
       " '달라': 717,\n",
       " '개발': 718,\n",
       " '카드': 719,\n",
       " '성공할까': 720,\n",
       " '아침햇발': 721,\n",
       " '부르나': 722,\n",
       " '월지급': 723,\n",
       " '버겁다추경안': 724,\n",
       " '늦어져': 725,\n",
       " '집값': 726,\n",
       " '현': 727,\n",
       " '지지율이': 728,\n",
       " '정부에': 729,\n",
       " '출신': 730,\n",
       " '살고': 731,\n",
       " '은': 732,\n",
       " '좋겠다': 733,\n",
       " '인간들': 734,\n",
       " '수사를': 735,\n",
       " '따라': 736,\n",
       " '서울시장이': 737,\n",
       " '위해서': 738,\n",
       " '니들': 739,\n",
       " '평당': 740,\n",
       " '정권의': 741,\n",
       " '저격': 742,\n",
       " '프레임': 743,\n",
       " '시즌': 744,\n",
       " '하네': 745,\n",
       " '애들': 746,\n",
       " '현장': 747,\n",
       " '년전': 748,\n",
       " '거짓말하고': 749,\n",
       " '수사': 750,\n",
       " '국민의힘으론': 751,\n",
       " '된다던': 752,\n",
       " '미풍': 753,\n",
       " '총장': 754,\n",
       " '논의': 755,\n",
       " '까지': 756,\n",
       " '볼': 757,\n",
       " '자격이': 758,\n",
       " '일단': 759,\n",
       " '없나': 760,\n",
       " '보면서': 761,\n",
       " '오세훈을': 762,\n",
       " '수가': 763,\n",
       " '당시': 764,\n",
       " '니들이': 765,\n",
       " '하니': 766,\n",
       " '조용히': 767,\n",
       " '것들이': 768,\n",
       " '처럼': 769,\n",
       " '미친': 770,\n",
       " '쉽게': 771,\n",
       " '그러면': 772,\n",
       " '돈으로': 773,\n",
       " '투표': 774,\n",
       " '본다': 775,\n",
       " '젊은': 776,\n",
       " '오세훈이는': 777,\n",
       " '너는': 778,\n",
       " '차기': 779,\n",
       " '저렇게': 780,\n",
       " '오세훈에': 781,\n",
       " '놈': 782,\n",
       " '그런데': 783,\n",
       " '사람들은': 784,\n",
       " '코로나': 785,\n",
       " '당연히': 786,\n",
       " '놈들': 787,\n",
       " '생각': 788,\n",
       " '그걸': 789,\n",
       " '세훈': 790,\n",
       " '시장은': 791,\n",
       " '위이언주': 792,\n",
       " '살아온': 793,\n",
       " '돼도': 794,\n",
       " '분노한': 795,\n",
       " '본인이': 796,\n",
       " '중요한': 797,\n",
       " '자기가': 798,\n",
       " '언급': 799,\n",
       " '서부전선상황실': 800,\n",
       " '프레임부동산': 801,\n",
       " '전쟁서울시장은': 802,\n",
       " '차지할까': 803,\n",
       " '후보다박영선': 804,\n",
       " '당당당': 805,\n",
       " '시작민주당': 806,\n",
       " '지지층': 807,\n",
       " '딛고': 808,\n",
       " '역전할까': 809,\n",
       " '거짓말에': 810,\n",
       " '생각하지': 811,\n",
       " '유세차': 812,\n",
       " '서초': 813,\n",
       " '실패한': 814,\n",
       " '변수는': 815,\n",
       " '수사청': 816,\n",
       " '대결': 817,\n",
       " '온갖': 818,\n",
       " '합시다': 819,\n",
       " '않으면': 820,\n",
       " '이번에': 821,\n",
       " '의': 822,\n",
       " '분': 823,\n",
       " '훨씬': 824,\n",
       " '생각을': 825,\n",
       " '자': 826,\n",
       " '노무현': 827,\n",
       " '나도': 828,\n",
       " '전에': 829,\n",
       " '했다': 830,\n",
       " '하는지': 831,\n",
       " '말하는': 832,\n",
       " '사는': 833,\n",
       " '박영선에게': 834,\n",
       " '미래를': 835,\n",
       " '한테': 836,\n",
       " '박근혜': 837,\n",
       " '내로남불': 838,\n",
       " '만원대': 839,\n",
       " '참으로': 840,\n",
       " '보선': 841,\n",
       " 'ㅋㅋㅋㅋㅋ': 842,\n",
       " '서울시장을': 843,\n",
       " '어차피': 844,\n",
       " '했던': 845,\n",
       " '없다는': 846,\n",
       " '그동안': 847,\n",
       " '해운대': 848,\n",
       " '도쿄아파트': 849,\n",
       " '책임을': 850,\n",
       " '연대': 851,\n",
       " '없네': 852,\n",
       " '화났다': 853,\n",
       " '역전': 854,\n",
       " '온라인': 855,\n",
       " '대폭발': 856,\n",
       " '고발뉴스': 857,\n",
       " '뉴스비평': 858,\n",
       " 'ㅎ': 859,\n",
       " '뉴스큐': 860,\n",
       " '감동실화오세훈': 861,\n",
       " '홧팅': 862,\n",
       " '생태탕': 863,\n",
       " '국가를': 864,\n",
       " '지정노무현': 865,\n",
       " '윤석열이야': 866,\n",
       " '하든우리는': 867,\n",
       " '차근차근': 868,\n",
       " '시민': 869,\n",
       " '곧': 870,\n",
       " '안철수가': 871,\n",
       " '하자': 872,\n",
       " '가고': 873,\n",
       " '마라': 874,\n",
       " '주고': 875,\n",
       " '이유가': 876,\n",
       " '정치는': 877,\n",
       " '당장': 878,\n",
       " '과거': 879,\n",
       " '거다': 880,\n",
       " '끝까지': 881,\n",
       " '하세요': 882,\n",
       " '있어야': 883,\n",
       " '로': 884,\n",
       " '세력과': 885,\n",
       " '검찰이': 886,\n",
       " '답이다': 887,\n",
       " '필승': 888,\n",
       " '봐라': 889,\n",
       " '국가': 890,\n",
       " '으로': 891,\n",
       " '대책': 892,\n",
       " '사퇴하라': 893,\n",
       " '네거티브': 894,\n",
       " '뉴스브리핑박영선은': 895,\n",
       " '있었길래오세훈': 896,\n",
       " '공방오늘밤': 897,\n",
       " '뭣이': 898,\n",
       " '기사를': 899,\n",
       " '가진': 900,\n",
       " '봤다': 901,\n",
       " '박찬수': 902,\n",
       " '윤': 903,\n",
       " '정권은': 904,\n",
       " '부산은': 905,\n",
       " '두고': 906,\n",
       " '오거돈': 907,\n",
       " '성추행으로': 908,\n",
       " '힘이': 909,\n",
       " '봐도': 910,\n",
       " '않을': 911,\n",
       " '아름다운': 912,\n",
       " '지지': 913,\n",
       " '박영선에': 914,\n",
       " '싶다': 915,\n",
       " '세': 916,\n",
       " '철수': 917,\n",
       " '완전히': 918,\n",
       " '정치에': 919,\n",
       " '뻔뻔한': 920,\n",
       " '안하고': 921,\n",
       " '물론': 922,\n",
       " '적폐': 923,\n",
       " '민주당의': 924,\n",
       " '것으로': 925,\n",
       " '능력이': 926,\n",
       " '있어': 927,\n",
       " '이를': 928,\n",
       " '가서': 929,\n",
       " '선거에서': 930,\n",
       " '나오는': 931,\n",
       " '찍어': 932,\n",
       " '없다고': 933,\n",
       " '대한민국을': 934,\n",
       " '무능한': 935,\n",
       " '나온다': 936,\n",
       " '할까': 937,\n",
       " '스스로': 938,\n",
       " '이번엔': 939,\n",
       " '난다': 940,\n",
       " '지지하는': 941,\n",
       " '입법': 942,\n",
       " '있으니': 943,\n",
       " '국민에게': 944,\n",
       " '그래': 945,\n",
       " '년대': 946,\n",
       " '주사파': 947,\n",
       " '아냐': 948,\n",
       " '만큼': 949,\n",
       " '건가': 950,\n",
       " '하루': 951,\n",
       " '그것도': 952,\n",
       " '돈을': 953,\n",
       " '심판': 954,\n",
       " '일을': 955,\n",
       " '지가': 956,\n",
       " '있다는': 957,\n",
       " '미래가': 958,\n",
       " '근데': 959,\n",
       " '없음': 960,\n",
       " '이해찬': 961,\n",
       " '입만': 962,\n",
       " '밤': 963,\n",
       " '오세훈후보님': 964,\n",
       " '초청박영선은': 965,\n",
       " '너도': 966,\n",
       " '같습니다': 967,\n",
       " '낫다': 968,\n",
       " '당선되면': 969,\n",
       " '하나로': 970,\n",
       " '지지를': 971,\n",
       " '서울시장은': 972,\n",
       " '절대로': 973,\n",
       " '그대로': 974,\n",
       " '전라도': 975,\n",
       " '특히': 976,\n",
       " 'ㅎㅎ': 977,\n",
       " '뭔가': 978,\n",
       " '대한민국이': 979,\n",
       " '개돼지로': 980,\n",
       " '쓴': 981,\n",
       " '문재인은': 982,\n",
       " '그렇지': 983,\n",
       " '서울에': 984,\n",
       " '재산': 985,\n",
       " '서울시민이': 986,\n",
       " '네가': 987,\n",
       " '나오면': 988,\n",
       " '가능성이': 989,\n",
       " '원래': 990,\n",
       " '이해가': 991,\n",
       " '정치인': 992,\n",
       " '됩니다': 993,\n",
       " '아들': 994,\n",
       " '검찰을': 995,\n",
       " '함': 996,\n",
       " '엄청': 997,\n",
       " '문제': 998,\n",
       " '주세요': 999,\n",
       " '박영선씨': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도가 높은 단어들로 구성\n",
    "num_word = 5000\n",
    "\n",
    "token = Tokenizer(num_words=num_word) # 토큰화 함수 지정\n",
    "token.fit_on_texts(docs) # 토큰화 함수에 문장 적용\n",
    "token.word_index # 각 단어에 매겨진 인덱스값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:04.027323Z",
     "start_time": "2021-05-13T16:37:04.023334Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36321"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token.word_index) # 전체 토큰 개수는 30503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:04.450193Z",
     "start_time": "2021-05-13T16:37:04.359435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[576, 83, 108, 15, 1356, 603, 817, 102, 4658, 1995, 1996, 904, 1704, 4659]\n",
      "재보궐  부산 김영춘  박형준 서울도 양자 대결 박영선은 정동영이 얻은 프로선에 머무를것 이후 정권은 몰락의 길 페달을 급속히 밟을것이다\n"
     ]
    }
   ],
   "source": [
    "# 앞서 만든 토큰의 인덱스로만 채워진 새로운 배열 생성\n",
    "X = token.texts_to_sequences(docs)\n",
    "print(X[0])  # 토큰화 후 인덱스로 채워진 새로운 배열\n",
    "print(docs[0])  # 토큰화 전 실제 문장의 배열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤗 문장별 토큰 개수 파악\n",
    "- 최대 토큰수를 찾기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:04.762169Z",
     "start_time": "2021-05-13T16:37:04.757371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 토큰으로 채워진 배열에서 최대 토큰수 찾기\n",
    "max_len = 0\n",
    "for i in X:\n",
    "    if max_len <= len(i):\n",
    "        max_len = len(i)\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:05.532632Z",
     "start_time": "2021-05-13T16:37:05.074007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 문장별 토큰 개수의 분포 확인\n",
    "total_token_count = []\n",
    "for i in X:\n",
    "    total_token_count.append(len(i))\n",
    "\n",
    "plt.hist( total_token_count, bins='auto' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:05.917862Z",
     "start_time": "2021-05-13T16:37:05.839042Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.boxplot(total_token_count)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:06.228999Z",
     "start_time": "2021-05-13T16:37:06.224012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[382, 136, 136, 128, 128, 128, 127, 127, 100, 100]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_token_count.sort(reverse=True)\n",
    "total_token_count[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 문장의 최대 토큰개수인 [955, 944, 842, 529, 228]개는 극단치라고 볼 수 있음\n",
    "- 토큰의 수가 [955, 944, 842, 529, 228]인 데이터 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:06.559118Z",
     "start_time": "2021-05-13T16:37:06.548147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len(tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.284626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.907304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>382.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       len(tokens)\n",
       "count  4332.000000\n",
       "mean     15.284626\n",
       "std      11.907304\n",
       "min       1.000000\n",
       "25%       9.000000\n",
       "50%      12.000000\n",
       "75%      18.000000\n",
       "max     382.000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 개수의 분포 확인\n",
    "pd.DataFrame(total_token_count, columns=['len(tokens)']).describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:06.987081Z",
     "start_time": "2021-05-13T16:37:06.979103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 개수의 95% 확인\n",
    "max_len = int(pd.Series(total_token_count).quantile(0.95) )\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **패딩(padding)** 시 맞춰주는 배열의 길이는 토큰의 최대길이 **163**로 맞춰준다.\n",
    "- **임베딩(embedding)** 시 배열을 압축할 때 사용하는 길이는 전체 문장의 95%가 포함되어 있는 토큰의 수인 **70**를 사용하여 압축한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 패딩(Padding)\n",
    "- **딥러닝 모델**에 **입력**을 하려면 **학습 데이터의 길이가 동일**해야함. 패딩은 길이를 맞춰주는 작업\n",
    "- 원하는 길이보다 짧은 부분은 숫자 0을 넣어 채우고,긴 데이터는 잘라서 같은 길이로 맞춘다.\n",
    "- 한 문장의 토큰 개수 합이 가장 많은 것으로 채우는 것이 좋다.\n",
    "- 따라서, 서로 다른 길이의 데이터를 **163**로 맞춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:10.282132Z",
     "start_time": "2021-05-13T16:37:10.255372Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  904, 1704, 4659],\n",
       "       [   0,    0,    0, ..., 1356,  603,  817],\n",
       "       [   0,    0,    0, ..., 1356,  603,  817],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  166, 2258,  745],\n",
       "       [   0,    0,    0, ...,  923, 2640, 1228],\n",
       "       [   0,    0,    0, ...,   80, 2441, 4560]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x = pad_sequences(X, max_len)\n",
    "padded_x # 배열의 길이가 맞춰짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 임베딩(Embedding)\n",
    "- **주어진 배열을 정해진 길이로 압축**\n",
    "- 텍스트를 원-핫 인코딩 시 벡터의 길이가 너무 길어지는 문제 해결\n",
    "\n",
    "\n",
    "- **Embedding**( 입력, 출력, 단어수 )\n",
    "    - **입력**: 총 몇 개의 단어 집합\n",
    "    - **출력**: 몇 개의 임베딩 결과 사용할 것인지(임의) -> 토큰 배열의 95%를 차지하는 70으로 설정\n",
    "    - **단어수**: 매번 입력될 단어 수는 몇 개로 할 것인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:15.631330Z",
     "start_time": "2021-05-13T16:21:15.626316Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36322"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 몇 개의 '인덱스'가 '입력' 되어야 하는지 정하기\n",
    "word_size = len(token.word_index) + 1 # 전체 단어 맨 앞에 0이 먼저 나와야 함.\n",
    "word_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 구조 생성/학습 및 저장\n",
    "- 1. 기본 딥러닝\n",
    "- 2. RNN(Recurrent Neural network, RNN)\n",
    "    - **여러개의 데이터가 순서대로 입력**되었을 때 앞서 입력받은 데이터를 **잠시 기억**해놓는 방법. 그리고 기억된 데이터가 얼마나 **중요한지를 판단**하여 **별도의 가중치**를 줘서 다음 데이터로 넘기게 됨\n",
    "    - **LSTM(Long Short Term Memory)**\n",
    "        - RNN의 기울기 소실 문제라는 단점을 보완한 방법\n",
    "        - 즉, 반복되기 직전에 다음 층으로 기억된 값을 넘길지 안 넘길지를 관리하는 단계 추가\n",
    "- 3. CNN과 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:15.646262Z",
     "start_time": "2021-05-13T16:21:15.632301Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists( f'/model/DNN_0514/num_words_{num_word}/'):\n",
    "    os.mkdir(f'/model/DNN_0514/num_words_{num_word}/' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:21:15.663218Z",
     "start_time": "2021-05-13T16:21:15.647260Z"
    }
   },
   "outputs": [],
   "source": [
    "word_size = num_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:22:59.856949Z",
     "start_time": "2021-05-13T16:21:15.665212Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "87/87 [==============================] - 2s 8ms/step - loss: 1.5126 - accuracy: 0.3262 - val_loss: 1.2423 - val_accuracy: 0.4098\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24234, saving model to model/DNN_0514\\1-1.24234139919281-0.40981242060661316.h5\n",
      "Epoch 2/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 1.2265 - accuracy: 0.5179 - val_loss: 1.1327 - val_accuracy: 0.6133\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.24234 to 1.13266, saving model to model/DNN_0514\\2-1.1326603889465332-0.6132755875587463.h5\n",
      "Epoch 3/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 1.0369 - accuracy: 0.7428 - val_loss: 0.9353 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.13266 to 0.93531, saving model to model/DNN_0514\\3-0.9353111982345581-0.6911976933479309.h5\n",
      "Epoch 4/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.7878 - accuracy: 0.8237 - val_loss: 0.7591 - val_accuracy: 0.7244\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.93531 to 0.75906, saving model to model/DNN_0514\\4-0.7590609192848206-0.7243867516517639.h5\n",
      "Epoch 5/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.5373 - accuracy: 0.8907 - val_loss: 0.6457 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.75906 to 0.64571, saving model to model/DNN_0514\\5-0.6457117199897766-0.7691197395324707.h5\n",
      "Epoch 6/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.3727 - accuracy: 0.9380 - val_loss: 0.5731 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.64571 to 0.57305, saving model to model/DNN_0514\\6-0.5730542540550232-0.8023087978363037.h5\n",
      "Epoch 7/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 0.9614 - val_loss: 0.5266 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.57305 to 0.52656, saving model to model/DNN_0514\\7-0.5265620946884155-0.8181818127632141.h5\n",
      "Epoch 8/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9739 - val_loss: 0.4951 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.52656 to 0.49514, saving model to model/DNN_0514\\8-0.4951430857181549-0.8311688303947449.h5\n",
      "Epoch 9/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9828 - val_loss: 0.4763 - val_accuracy: 0.8326\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.49514 to 0.47632, saving model to model/DNN_0514\\9-0.47631779313087463-0.8326118588447571.h5\n",
      "Epoch 10/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.9845 - val_loss: 0.4623 - val_accuracy: 0.8355\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47632 to 0.46230, saving model to model/DNN_0514\\10-0.46230044960975647-0.8354978561401367.h5\n",
      "Epoch 11/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0944 - accuracy: 0.9859 - val_loss: 0.4522 - val_accuracy: 0.8355\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.46230 to 0.45221, saving model to model/DNN_0514\\11-0.4522109627723694-0.8354978561401367.h5\n",
      "Epoch 12/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0766 - accuracy: 0.9884 - val_loss: 0.4488 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.45221 to 0.44877, saving model to model/DNN_0514\\12-0.4487742781639099-0.8340548276901245.h5\n",
      "Epoch 13/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9916 - val_loss: 0.4454 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.44877 to 0.44540, saving model to model/DNN_0514\\13-0.44540154933929443-0.8340548276901245.h5\n",
      "Epoch 14/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0573 - accuracy: 0.9911 - val_loss: 0.4398 - val_accuracy: 0.8384\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.44540 to 0.43983, saving model to model/DNN_0514\\14-0.4398271441459656-0.8383838534355164.h5\n",
      "Epoch 15/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.9928 - val_loss: 0.4384 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.43983 to 0.43838, saving model to model/DNN_0514\\15-0.4383774697780609-0.8340548276901245.h5\n",
      "Epoch 16/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9941 - val_loss: 0.4358 - val_accuracy: 0.8326\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.43838 to 0.43583, saving model to model/DNN_0514\\16-0.4358274042606354-0.8326118588447571.h5\n",
      "Epoch 17/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.9917 - val_loss: 0.4368 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43583\n",
      "Epoch 18/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.9933 - val_loss: 0.4362 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43583\n",
      "Epoch 19/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9960 - val_loss: 0.4376 - val_accuracy: 0.8355\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43583\n",
      "Epoch 20/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9964 - val_loss: 0.4389 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43583\n",
      "Epoch 21/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9953 - val_loss: 0.4413 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43583\n",
      "Epoch 22/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.9959 - val_loss: 0.4396 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.43583\n",
      "Epoch 23/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0221 - accuracy: 0.9960 - val_loss: 0.4428 - val_accuracy: 0.8297\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.43583\n",
      "Epoch 24/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.9957 - val_loss: 0.4431 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.43583\n",
      "Epoch 25/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9945 - val_loss: 0.4449 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43583\n",
      "Epoch 26/1500\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 0.0187 - accuracy: 0.9950 - val_loss: 0.4453 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43583\n",
      "Epoch 27/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.4484 - val_accuracy: 0.8254\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.43583\n",
      "Epoch 28/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9967 - val_loss: 0.4521 - val_accuracy: 0.8240\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.43583\n",
      "Epoch 29/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 0.4515 - val_accuracy: 0.8254\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.43583\n",
      "Epoch 30/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0138 - accuracy: 0.9974 - val_loss: 0.4555 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.43583\n",
      "Epoch 31/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9975 - val_loss: 0.4579 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.43583\n",
      "Epoch 32/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9938 - val_loss: 0.4562 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.43583\n",
      "Epoch 33/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.4579 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.43583\n",
      "Epoch 34/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.4591 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.43583\n",
      "Epoch 35/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 0.9934 - val_loss: 0.4678 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.43583\n",
      "Epoch 36/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9953 - val_loss: 0.4623 - val_accuracy: 0.8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss did not improve from 0.43583\n",
      "Epoch 37/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9921 - val_loss: 0.4661 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.43583\n",
      "Epoch 38/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 0.4677 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43583\n",
      "Epoch 39/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9952 - val_loss: 0.4756 - val_accuracy: 0.8196\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.43583\n",
      "Epoch 40/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9962 - val_loss: 0.4746 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43583\n",
      "Epoch 41/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9941 - val_loss: 0.4745 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.43583\n",
      "Epoch 42/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0153 - accuracy: 0.9927 - val_loss: 0.4791 - val_accuracy: 0.8196\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.43583\n",
      "Epoch 43/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9923 - val_loss: 0.4779 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.43583\n",
      "Epoch 44/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9955 - val_loss: 0.4782 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.43583\n",
      "Epoch 45/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9963 - val_loss: 0.4762 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.43583\n",
      "Epoch 46/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 0.4849 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.43583\n",
      "Epoch 47/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9952 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.43583\n",
      "Epoch 48/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9941 - val_loss: 0.4848 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.43583\n",
      "Epoch 49/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9936 - val_loss: 0.4887 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.43583\n",
      "Epoch 50/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9932 - val_loss: 0.4881 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.43583\n",
      "Epoch 51/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9937 - val_loss: 0.4872 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.43583\n",
      "Epoch 52/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9927 - val_loss: 0.4880 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.43583\n",
      "Epoch 53/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9966 - val_loss: 0.4985 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.43583\n",
      "Epoch 54/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9902 - val_loss: 0.4989 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.43583\n",
      "Epoch 55/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9937 - val_loss: 0.4947 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.43583\n",
      "Epoch 56/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9942 - val_loss: 0.4963 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.43583\n",
      "Epoch 57/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9964 - val_loss: 0.5008 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.43583\n",
      "Epoch 58/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9944 - val_loss: 0.5008 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.43583\n",
      "Epoch 59/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9938 - val_loss: 0.5013 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.43583\n",
      "Epoch 60/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9929 - val_loss: 0.5073 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.43583\n",
      "Epoch 61/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.9949 - val_loss: 0.5096 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.43583\n",
      "Epoch 62/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.9945 - val_loss: 0.5081 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.43583\n",
      "Epoch 63/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9950 - val_loss: 0.5082 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.43583\n",
      "Epoch 64/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.9937 - val_loss: 0.5177 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.43583\n",
      "Epoch 65/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 0.5132 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.43583\n",
      "Epoch 66/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9962 - val_loss: 0.5180 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.43583\n",
      "Epoch 67/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9955 - val_loss: 0.5141 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.43583\n",
      "Epoch 68/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9928 - val_loss: 0.5229 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.43583\n",
      "Epoch 69/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.5177 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.43583\n",
      "Epoch 70/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.5185 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.43583\n",
      "Epoch 71/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9915 - val_loss: 0.5251 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.43583\n",
      "Epoch 72/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9942 - val_loss: 0.5279 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.43583\n",
      "Epoch 73/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9964 - val_loss: 0.5246 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.43583\n",
      "Epoch 74/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 0.5303 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.43583\n",
      "Epoch 75/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9926 - val_loss: 0.5292 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.43583\n",
      "Epoch 76/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 0.9941 - val_loss: 0.5306 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.43583\n",
      "Epoch 77/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9936 - val_loss: 0.5381 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.43583\n",
      "Epoch 78/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.5364 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.43583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.9931 - val_loss: 0.5369 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.43583\n",
      "Epoch 80/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 0.9949 - val_loss: 0.5360 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.43583\n",
      "Epoch 81/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.9951 - val_loss: 0.5407 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.43583\n",
      "Epoch 82/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.9968 - val_loss: 0.5387 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.43583\n",
      "Epoch 83/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9936 - val_loss: 0.5438 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.43583\n",
      "Epoch 84/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 0.9955 - val_loss: 0.5473 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.43583\n",
      "Epoch 85/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9968 - val_loss: 0.5503 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.43583\n",
      "Epoch 86/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9916 - val_loss: 0.5514 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.43583\n",
      "Epoch 87/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9953 - val_loss: 0.5498 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.43583\n",
      "Epoch 88/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9939 - val_loss: 0.5519 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.43583\n",
      "Epoch 89/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.9937 - val_loss: 0.5487 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.43583\n",
      "Epoch 90/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 0.9970 - val_loss: 0.5505 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.43583\n",
      "Epoch 91/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9942 - val_loss: 0.5519 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.43583\n",
      "Epoch 92/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9938 - val_loss: 0.5549 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.43583\n",
      "Epoch 93/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.9964 - val_loss: 0.5580 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.43583\n",
      "Epoch 94/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9938 - val_loss: 0.5647 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.43583\n",
      "Epoch 95/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9953 - val_loss: 0.5605 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.43583\n",
      "Epoch 96/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9942 - val_loss: 0.5616 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.43583\n",
      "Epoch 97/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9973 - val_loss: 0.5641 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.43583\n",
      "Epoch 98/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9963 - val_loss: 0.5642 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.43583\n",
      "Epoch 99/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9945 - val_loss: 0.5688 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.43583\n",
      "Epoch 100/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9959 - val_loss: 0.5687 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.43583\n",
      "Epoch 101/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9950 - val_loss: 0.5707 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.43583\n",
      "Epoch 102/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9923 - val_loss: 0.5738 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.43583\n",
      "Epoch 103/1500\n",
      "87/87 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 0.9938 - val_loss: 0.5773 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.43583\n",
      "Epoch 104/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9961 - val_loss: 0.5773 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.43583\n",
      "Epoch 105/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9944 - val_loss: 0.5768 - val_accuracy: 0.8095\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.43583\n",
      "Epoch 106/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9962 - val_loss: 0.5805 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.43583\n",
      "Epoch 107/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.5830 - val_accuracy: 0.8066\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.43583\n",
      "Epoch 108/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.9936 - val_loss: 0.5820 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.43583\n",
      "Epoch 109/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9943 - val_loss: 0.5812 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.43583\n",
      "Epoch 110/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0121 - accuracy: 0.9911 - val_loss: 0.5864 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.43583\n",
      "Epoch 111/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.5933 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.43583\n",
      "Epoch 112/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9969 - val_loss: 0.5924 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.43583\n",
      "Epoch 113/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9920 - val_loss: 0.5958 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.43583\n",
      "Epoch 114/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 0.9943 - val_loss: 0.5975 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.43583\n",
      "Epoch 115/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0081 - accuracy: 0.9961 - val_loss: 0.5965 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.43583\n",
      "Epoch 116/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9967 - val_loss: 0.5995 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.43583\n",
      "Epoch 117/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 0.9950 - val_loss: 0.6029 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.43583\n",
      "Epoch 118/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 0.9947 - val_loss: 0.6037 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.43583\n",
      "Epoch 119/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9949 - val_loss: 0.6061 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.43583\n",
      "Epoch 120/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9946 - val_loss: 0.6063 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.43583\n",
      "Epoch 121/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.9962 - val_loss: 0.6078 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.43583\n",
      "Epoch 122/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.6150 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.43583\n",
      "Epoch 123/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9942 - val_loss: 0.6124 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.43583\n",
      "Epoch 124/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 0.9963 - val_loss: 0.6102 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.43583\n",
      "Epoch 125/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9946 - val_loss: 0.6220 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.43583\n",
      "Epoch 126/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9956 - val_loss: 0.6193 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.43583\n",
      "Epoch 127/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 0.6243 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.43583\n",
      "Epoch 128/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 0.9938 - val_loss: 0.6204 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.43583\n",
      "Epoch 129/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.9944 - val_loss: 0.6286 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.43583\n",
      "Epoch 130/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9929 - val_loss: 0.6328 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.43583\n",
      "Epoch 131/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9947 - val_loss: 0.6275 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.43583\n",
      "Epoch 132/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 0.9936 - val_loss: 0.6296 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.43583\n",
      "Epoch 133/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9939 - val_loss: 0.6360 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.43583\n",
      "Epoch 134/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 0.9946 - val_loss: 0.6347 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.43583\n",
      "Epoch 135/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 0.9920 - val_loss: 0.6408 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.43583\n",
      "Epoch 136/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9964 - val_loss: 0.6436 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.43583\n",
      "Epoch 137/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9955 - val_loss: 0.6422 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.43583\n",
      "Epoch 138/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9938 - val_loss: 0.6453 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.43583\n",
      "Epoch 139/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.9934 - val_loss: 0.6465 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.43583\n",
      "Epoch 140/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9958 - val_loss: 0.6480 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.43583\n",
      "Epoch 141/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9964 - val_loss: 0.6536 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.43583\n",
      "Epoch 142/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9946 - val_loss: 0.6561 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.43583\n",
      "Epoch 143/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9957 - val_loss: 0.6553 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.43583\n",
      "Epoch 144/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9949 - val_loss: 0.6570 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.43583\n",
      "Epoch 145/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 0.9945 - val_loss: 0.6634 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.43583\n",
      "Epoch 146/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 0.6675 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.43583\n",
      "Epoch 147/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 0.9959 - val_loss: 0.6695 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.43583\n",
      "Epoch 148/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9942 - val_loss: 0.6728 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.43583\n",
      "Epoch 149/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 0.9960 - val_loss: 0.6723 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.43583\n",
      "Epoch 150/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9952 - val_loss: 0.6745 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.43583\n",
      "Epoch 151/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0054 - accuracy: 0.9976 - val_loss: 0.6765 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.43583\n",
      "Epoch 152/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9934 - val_loss: 0.6806 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.43583\n",
      "Epoch 153/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9936 - val_loss: 0.6866 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.43583\n",
      "Epoch 154/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9930 - val_loss: 0.6863 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.43583\n",
      "Epoch 155/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9921 - val_loss: 0.6910 - val_accuracy: 0.7951\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.43583\n",
      "Epoch 156/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 0.9962 - val_loss: 0.6915 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.43583\n",
      "Epoch 157/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9934 - val_loss: 0.6972 - val_accuracy: 0.7937\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.43583\n",
      "Epoch 158/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9937 - val_loss: 0.6953 - val_accuracy: 0.7937\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.43583\n",
      "Epoch 159/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9948 - val_loss: 0.6991 - val_accuracy: 0.7937\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.43583\n",
      "Epoch 160/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 0.9956 - val_loss: 0.7036 - val_accuracy: 0.7937\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.43583\n",
      "Epoch 161/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9938 - val_loss: 0.7053 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.43583\n",
      "Epoch 162/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.9947 - val_loss: 0.7087 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.43583\n",
      "Epoch 163/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0053 - accuracy: 0.9978 - val_loss: 0.7082 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.43583\n",
      "Epoch 164/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9956 - val_loss: 0.7127 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.43583\n",
      "Epoch 165/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9944 - val_loss: 0.7160 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.43583\n",
      "Epoch 166/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9929 - val_loss: 0.7142 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.43583\n",
      "Epoch 167/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9959 - val_loss: 0.7202 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.43583\n",
      "Epoch 168/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9963 - val_loss: 0.7248 - val_accuracy: 0.7893\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.43583\n",
      "Epoch 169/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9959 - val_loss: 0.7293 - val_accuracy: 0.7893\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.43583\n",
      "Epoch 170/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9941 - val_loss: 0.7277 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.43583\n",
      "Epoch 171/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9947 - val_loss: 0.7332 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.43583\n",
      "Epoch 172/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9940 - val_loss: 0.7380 - val_accuracy: 0.7864\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.43583\n",
      "Epoch 173/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9960 - val_loss: 0.7376 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.43583\n",
      "Epoch 174/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9954 - val_loss: 0.7452 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.43583\n",
      "Epoch 175/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9931 - val_loss: 0.7447 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.43583\n",
      "Epoch 176/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9966 - val_loss: 0.7504 - val_accuracy: 0.7864\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.43583\n",
      "Epoch 177/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9964 - val_loss: 0.7522 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.43583\n",
      "Epoch 178/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9946 - val_loss: 0.7536 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.43583\n",
      "Epoch 179/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9950 - val_loss: 0.7585 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.43583\n",
      "Epoch 180/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9956 - val_loss: 0.7598 - val_accuracy: 0.7835\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.43583\n",
      "Epoch 181/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 0.9942 - val_loss: 0.7678 - val_accuracy: 0.7821\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.43583\n",
      "Epoch 182/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 0.9929 - val_loss: 0.7702 - val_accuracy: 0.7821\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.43583\n",
      "Epoch 183/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 0.9945 - val_loss: 0.7705 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.43583\n",
      "Epoch 184/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 0.9953 - val_loss: 0.7747 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.43583\n",
      "Epoch 185/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9933 - val_loss: 0.7773 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.43583\n",
      "Epoch 186/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9966 - val_loss: 0.7802 - val_accuracy: 0.7763\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.43583\n",
      "Epoch 187/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9959 - val_loss: 0.7823 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.43583\n",
      "Epoch 188/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9938 - val_loss: 0.7856 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.43583\n",
      "Epoch 189/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9946 - val_loss: 0.7878 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.43583\n",
      "Epoch 190/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9930 - val_loss: 0.7899 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.43583\n",
      "Epoch 191/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9922 - val_loss: 0.7939 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.43583\n",
      "Epoch 192/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.7994 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.43583\n",
      "Epoch 193/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9916 - val_loss: 0.8040 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.43583\n",
      "Epoch 194/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 0.9954 - val_loss: 0.8078 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.43583\n",
      "Epoch 195/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.9962 - val_loss: 0.8095 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.43583\n",
      "Epoch 196/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0103 - accuracy: 0.9959 - val_loss: 0.8160 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.43583\n",
      "Epoch 197/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 0.9950 - val_loss: 0.8180 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.43583\n",
      "Epoch 198/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9949 - val_loss: 0.8247 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.43583\n",
      "Epoch 199/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9932 - val_loss: 0.8284 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.43583\n",
      "Epoch 200/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9945 - val_loss: 0.8311 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.43583\n",
      "Epoch 201/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 0.9947 - val_loss: 0.8351 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.43583\n",
      "Epoch 202/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 0.9961 - val_loss: 0.8376 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.43583\n",
      "Epoch 203/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9929 - val_loss: 0.8396 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.43583\n",
      "Epoch 204/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9945 - val_loss: 0.8412 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.43583\n",
      "Epoch 205/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 0.9933 - val_loss: 0.8471 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.43583\n",
      "Epoch 206/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9920 - val_loss: 0.8507 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.43583\n",
      "Epoch 207/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 0.9946 - val_loss: 0.8572 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.43583\n",
      "Epoch 208/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9940 - val_loss: 0.8621 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.43583\n",
      "Epoch 209/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9919 - val_loss: 0.8653 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.43583\n",
      "Epoch 210/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 0.9967 - val_loss: 0.8696 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.43583\n",
      "Epoch 211/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9932 - val_loss: 0.8728 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.43583\n",
      "Epoch 212/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 0.9956 - val_loss: 0.8757 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.43583\n",
      "Epoch 213/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9960 - val_loss: 0.8801 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.43583\n",
      "Epoch 214/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 0.9955 - val_loss: 0.8817 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.43583\n",
      "Epoch 215/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 0.9968 - val_loss: 0.8856 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.43583\n",
      "Epoch 216/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 0.9958 - val_loss: 0.8868 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.43583\n",
      "Epoch 217/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9952 - val_loss: 0.8891 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.43583\n",
      "Epoch 218/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 0.9959 - val_loss: 0.8966 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.43583\n",
      "Epoch 219/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9918 - val_loss: 0.8993 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.43583\n",
      "Epoch 220/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9924 - val_loss: 0.9035 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.43583\n",
      "Epoch 221/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 0.9945 - val_loss: 0.9047 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.43583\n",
      "Epoch 222/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.9076 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.43583\n",
      "Epoch 223/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.9958 - val_loss: 0.9110 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.43583\n",
      "Epoch 224/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0098 - accuracy: 0.9942 - val_loss: 0.9126 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.43583\n",
      "Epoch 225/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9952 - val_loss: 0.9148 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.43583\n",
      "Epoch 226/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9947 - val_loss: 0.9236 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.43583\n",
      "Epoch 227/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.9974 - val_loss: 0.9240 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.43583\n",
      "Epoch 228/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.9948 - val_loss: 0.9294 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.43583\n",
      "Epoch 229/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9938 - val_loss: 0.9317 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.43583\n",
      "Epoch 230/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.9961 - val_loss: 0.9334 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.43583\n",
      "Epoch 231/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9947 - val_loss: 0.9367 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.43583\n",
      "Epoch 232/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9951 - val_loss: 0.9386 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.43583\n",
      "Epoch 233/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.9965 - val_loss: 0.9387 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.43583\n",
      "Epoch 234/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.9968 - val_loss: 0.9473 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.43583\n",
      "Epoch 235/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9959 - val_loss: 0.9517 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.43583\n",
      "Epoch 236/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9944 - val_loss: 0.9552 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.43583\n",
      "Epoch 237/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9931 - val_loss: 0.9567 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.43583\n",
      "Epoch 238/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0107 - accuracy: 0.9935 - val_loss: 0.9601 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.43583\n",
      "Epoch 239/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9956 - val_loss: 0.9636 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.43583\n",
      "Epoch 240/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9940 - val_loss: 0.9684 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.43583\n",
      "Epoch 241/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9942 - val_loss: 0.9648 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.43583\n",
      "Epoch 242/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9963 - val_loss: 0.9732 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.43583\n",
      "Epoch 243/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.9956 - val_loss: 0.9751 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.43583\n",
      "Epoch 244/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.9943 - val_loss: 0.9790 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.43583\n",
      "Epoch 245/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9956 - val_loss: 0.9831 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.43583\n",
      "Epoch 246/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9953 - val_loss: 0.9897 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.43583\n",
      "Epoch 247/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9955 - val_loss: 0.9872 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.43583\n",
      "Epoch 248/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9951 - val_loss: 0.9911 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.43583\n",
      "Epoch 249/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9929 - val_loss: 0.9957 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.43583\n",
      "Epoch 250/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9918 - val_loss: 0.9977 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.43583\n",
      "Epoch 251/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9962 - val_loss: 1.0012 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.43583\n",
      "Epoch 252/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 0.9929 - val_loss: 1.0036 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.43583\n",
      "Epoch 253/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.9947 - val_loss: 1.0064 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.43583\n",
      "Epoch 254/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9964 - val_loss: 1.0084 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.43583\n",
      "Epoch 255/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 0.9949 - val_loss: 1.0134 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.43583\n",
      "Epoch 256/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9967 - val_loss: 1.0152 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.43583\n",
      "Epoch 257/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 1.0178 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.43583\n",
      "Epoch 258/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9925 - val_loss: 1.0214 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.43583\n",
      "Epoch 259/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 0.9956 - val_loss: 1.0237 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.43583\n",
      "Epoch 260/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 1.0240 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.43583\n",
      "Epoch 261/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 0.9959 - val_loss: 1.0266 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.43583\n",
      "Epoch 262/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9967 - val_loss: 1.0289 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.43583\n",
      "Epoch 263/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 0.9967 - val_loss: 1.0316 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.43583\n",
      "Epoch 264/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.9971 - val_loss: 1.0348 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.43583\n",
      "Epoch 265/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9938 - val_loss: 1.0394 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.43583\n",
      "Epoch 266/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 0.9965 - val_loss: 1.0412 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.43583\n",
      "Epoch 267/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9957 - val_loss: 1.0396 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.43583\n",
      "Epoch 268/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 0.9956 - val_loss: 1.0410 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.43583\n",
      "Epoch 269/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9961 - val_loss: 1.0476 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.43583\n",
      "Epoch 270/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9965 - val_loss: 1.0536 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.43583\n",
      "Epoch 271/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9924 - val_loss: 1.0569 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.43583\n",
      "Epoch 272/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9960 - val_loss: 1.0590 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.43583\n",
      "Epoch 273/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 0.9948 - val_loss: 1.0633 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.43583\n",
      "Epoch 274/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9947 - val_loss: 1.0637 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.43583\n",
      "Epoch 275/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 0.9958 - val_loss: 1.0722 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.43583\n",
      "Epoch 276/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 0.9938 - val_loss: 1.0731 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.43583\n",
      "Epoch 277/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9911 - val_loss: 1.0742 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.43583\n",
      "Epoch 278/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 0.9960 - val_loss: 1.0775 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.43583\n",
      "Epoch 279/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9938 - val_loss: 1.0799 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.43583\n",
      "Epoch 280/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9944 - val_loss: 1.0845 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.43583\n",
      "Epoch 281/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 0.9960 - val_loss: 1.0889 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.43583\n",
      "Epoch 282/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 0.9930 - val_loss: 1.0912 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.43583\n",
      "Epoch 283/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9960 - val_loss: 1.0952 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.43583\n",
      "Epoch 284/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.9927 - val_loss: 1.1000 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.43583\n",
      "Epoch 285/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9944 - val_loss: 1.1075 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.43583\n",
      "Epoch 286/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9947 - val_loss: 1.1101 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.43583\n",
      "Epoch 287/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9948 - val_loss: 1.1117 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.43583\n",
      "Epoch 288/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9923 - val_loss: 1.1139 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.43583\n",
      "Epoch 289/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.9922 - val_loss: 1.1170 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.43583\n",
      "Epoch 290/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9955 - val_loss: 1.1199 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.43583\n",
      "Epoch 291/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9970 - val_loss: 1.1291 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.43583\n",
      "Epoch 292/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0091 - accuracy: 0.9951 - val_loss: 1.1266 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.43583\n",
      "Epoch 293/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9965 - val_loss: 1.1306 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.43583\n",
      "Epoch 294/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9966 - val_loss: 1.1374 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.43583\n",
      "Epoch 295/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 1.1421 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.43583\n",
      "Epoch 296/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9953 - val_loss: 1.1451 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.43583\n",
      "Epoch 297/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9950 - val_loss: 1.1417 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.43583\n",
      "Epoch 298/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9966 - val_loss: 1.1419 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.43583\n",
      "Epoch 299/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9938 - val_loss: 1.1474 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.43583\n",
      "Epoch 300/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9971 - val_loss: 1.1494 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.43583\n",
      "Epoch 301/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.9949 - val_loss: 1.1537 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.43583\n",
      "Epoch 302/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9953 - val_loss: 1.1556 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.43583\n",
      "Epoch 303/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9951 - val_loss: 1.1599 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.43583\n",
      "Epoch 304/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9950 - val_loss: 1.1568 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.43583\n",
      "Epoch 305/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 0.9955 - val_loss: 1.1588 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.43583\n",
      "Epoch 306/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0085 - accuracy: 0.9963 - val_loss: 1.1610 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.43583\n",
      "Epoch 307/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9950 - val_loss: 1.1631 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.43583\n",
      "Epoch 308/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 0.9941 - val_loss: 1.1680 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.43583\n",
      "Epoch 309/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 0.9942 - val_loss: 1.1704 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.43583\n",
      "Epoch 310/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.9956 - val_loss: 1.1731 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.43583\n",
      "Epoch 311/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9932 - val_loss: 1.1720 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.43583\n",
      "Epoch 312/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9951 - val_loss: 1.1752 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.43583\n",
      "Epoch 313/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9948 - val_loss: 1.1776 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.43583\n",
      "Epoch 314/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9959 - val_loss: 1.1804 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.43583\n",
      "Epoch 315/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9944 - val_loss: 1.1850 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.43583\n",
      "Epoch 316/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9941 - val_loss: 1.1886 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.43583\n"
     ]
    }
   ],
   "source": [
    "# DNN 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding( num_word, 8 , input_length=max_len ) )\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='softmax')) # 다중분류\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile( loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "import os\n",
    "\n",
    "if os.path.exists( f'/model/DNN_0514/num_words_{num_word}/'):\n",
    "    os.mkdir(f'/model/DNN_0514/num_words_{num_word}/' )\n",
    "    \n",
    "model_path = 'model/DNN_0514/{epoch}-{val_loss}-{val_accuracy}.h5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', \n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',patience=300)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_x, \n",
    "                                                     y_encoded, \n",
    "                                                     test_size=0.2, \n",
    "                                                     stratify=y_encoded)\n",
    "\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit( X_train, y_train, validation_split=0.2,\n",
    "                    epochs=1500,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:22:59.861935Z",
     "start_time": "2021-05-13T16:22:59.857946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 43, 8)             120000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 344)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1725      \n",
      "=================================================================\n",
      "Total params: 121,725\n",
      "Trainable params: 121,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:23:00.094908Z",
     "start_time": "2021-05-13T16:22:59.864929Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트셋으로 실험 결과의 오차값을 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋으로 측정한 정확도의 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# x값을 지정하고 그래프로 확인\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.title('[Figure] DNN 모델의 결과')\n",
    "plt.plot( x_len, y_vloss, 'o', c='red', markersize=3, label='TestSet Loss')\n",
    "plt.plot(x_len, y_acc, 'o', c='blue', markersize=3, label='TrainSet Acuraccy')\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.savefig(f'DNN_num_words_{num_word}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:23:00.430014Z",
     "start_time": "2021-05-13T16:23:00.095905Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9460\n",
      "0.9460317492485046\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.0094 - accuracy: 0.7785\n",
      "0.7785466909408569\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 정확도\n",
    "print(model.evaluate(X_train,y_train)[1])\n",
    "\n",
    "# 테스트 데이터 정확도\n",
    "print(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:23:00.435000Z",
     "start_time": "2021-05-13T16:23:00.431011Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불러올 총 단어의 수\n",
    "word_size = num_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:26:43.625250Z",
     "start_time": "2021-05-13T16:23:00.435998Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "67/67 [==============================] - 3s 14ms/step - loss: 1.4441 - accuracy: 0.3811 - val_loss: 1.1386 - val_accuracy: 0.5308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.13864, saving model to model/LSTM_0514\\1-1.138643741607666-0.5307692289352417.h5\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.9800 - accuracy: 0.5983 - val_loss: 0.7331 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.13864 to 0.73315, saving model to model/LSTM_0514\\2-0.7331494688987732-0.69010990858078.h5\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.5859 - accuracy: 0.7565 - val_loss: 0.5651 - val_accuracy: 0.7901\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.73315 to 0.56512, saving model to model/LSTM_0514\\3-0.5651189684867859-0.7901098728179932.h5\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.2982 - accuracy: 0.9202 - val_loss: 0.4574 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56512 to 0.45738, saving model to model/LSTM_0514\\4-0.45737773180007935-0.8505494594573975.h5\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.1620 - accuracy: 0.9607 - val_loss: 0.4374 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45738 to 0.43737, saving model to model/LSTM_0514\\5-0.4373718798160553-0.8483516573905945.h5\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0934 - accuracy: 0.9752 - val_loss: 0.4873 - val_accuracy: 0.8352\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43737\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0775 - accuracy: 0.9776 - val_loss: 0.4483 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43737\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0484 - accuracy: 0.9898 - val_loss: 0.4607 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43737\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0494 - accuracy: 0.9886 - val_loss: 0.4612 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43737\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0427 - accuracy: 0.9901 - val_loss: 0.4882 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43737\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0347 - accuracy: 0.9905 - val_loss: 0.5062 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43737\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0307 - accuracy: 0.9931 - val_loss: 0.4875 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43737\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0286 - accuracy: 0.9903 - val_loss: 0.5174 - val_accuracy: 0.8451\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43737\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0279 - accuracy: 0.9902 - val_loss: 0.5421 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43737\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0190 - accuracy: 0.9948 - val_loss: 0.5251 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43737\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0222 - accuracy: 0.9922 - val_loss: 0.5615 - val_accuracy: 0.8604\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43737\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0149 - accuracy: 0.9972 - val_loss: 0.5809 - val_accuracy: 0.8352\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43737\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 0.5808 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43737\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.5793 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43737\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.5867 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43737\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0183 - accuracy: 0.9922 - val_loss: 0.6125 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43737\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0201 - accuracy: 0.9903 - val_loss: 0.6027 - val_accuracy: 0.8407\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.43737\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0135 - accuracy: 0.9945 - val_loss: 0.5954 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.43737\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0169 - accuracy: 0.9939 - val_loss: 0.6269 - val_accuracy: 0.8363\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.43737\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0132 - accuracy: 0.9941 - val_loss: 0.6091 - val_accuracy: 0.8374\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43737\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0129 - accuracy: 0.9936 - val_loss: 0.5983 - val_accuracy: 0.8451\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43737\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.6292 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.43737\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0100 - accuracy: 0.9948 - val_loss: 0.6152 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.43737\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.0113 - accuracy: 0.9941 - val_loss: 0.6219 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.43737\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.0098 - accuracy: 0.9952 - val_loss: 0.6359 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.43737\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0150 - accuracy: 0.9920 - val_loss: 0.6775 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.43737\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0102 - accuracy: 0.9944 - val_loss: 0.6615 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.43737\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0107 - accuracy: 0.9956 - val_loss: 0.6890 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.43737\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0123 - accuracy: 0.9939 - val_loss: 0.7031 - val_accuracy: 0.8407\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.43737\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0099 - accuracy: 0.9957 - val_loss: 0.6838 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.43737\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0100 - accuracy: 0.9948 - val_loss: 0.7008 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.43737\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0104 - accuracy: 0.9944 - val_loss: 0.7013 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.43737\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.6944 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.43737\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 0.9965 - val_loss: 0.7484 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.43737\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0111 - accuracy: 0.9942 - val_loss: 0.7195 - val_accuracy: 0.8352\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.43737\n",
      "Epoch 41/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0084 - accuracy: 0.9933 - val_loss: 0.7359 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.43737\n",
      "Epoch 42/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0106 - accuracy: 0.9926 - val_loss: 0.7498 - val_accuracy: 0.8363\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.43737\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0109 - accuracy: 0.9927 - val_loss: 0.7372 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.43737\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0106 - accuracy: 0.9945 - val_loss: 0.7570 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.43737\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0120 - accuracy: 0.9935 - val_loss: 0.8269 - val_accuracy: 0.8341\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.43737\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0077 - accuracy: 0.9966 - val_loss: 0.8122 - val_accuracy: 0.8297\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.43737\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0105 - accuracy: 0.9917 - val_loss: 0.8083 - val_accuracy: 0.8352\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.43737\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0062 - accuracy: 0.9965 - val_loss: 0.8113 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.43737\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0137 - accuracy: 0.9927 - val_loss: 0.7950 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.43737\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0106 - accuracy: 0.9936 - val_loss: 0.8372 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.43737\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0082 - accuracy: 0.9949 - val_loss: 0.8287 - val_accuracy: 0.8352\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.43737\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0088 - accuracy: 0.9943 - val_loss: 0.8333 - val_accuracy: 0.8451\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.43737\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0122 - accuracy: 0.9916 - val_loss: 0.8520 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.43737\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0113 - accuracy: 0.9940 - val_loss: 0.8694 - val_accuracy: 0.8407\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.43737\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0102 - accuracy: 0.9925 - val_loss: 0.8996 - val_accuracy: 0.8297\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.43737\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0075 - accuracy: 0.9950 - val_loss: 0.8553 - val_accuracy: 0.8297\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.43737\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 0.9959 - val_loss: 0.8923 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.43737\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0094 - accuracy: 0.9936 - val_loss: 0.9156 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.43737\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 0.9952 - val_loss: 0.9020 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.43737\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0071 - accuracy: 0.9960 - val_loss: 0.8847 - val_accuracy: 0.8407\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.43737\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.9474 - val_accuracy: 0.8319\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.43737\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0116 - accuracy: 0.9915 - val_loss: 0.9194 - val_accuracy: 0.8330\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.43737\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0102 - accuracy: 0.9937 - val_loss: 0.9052 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.43737\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0083 - accuracy: 0.9954 - val_loss: 0.9992 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.43737\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0078 - accuracy: 0.9962 - val_loss: 0.9237 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.43737\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 1.0007 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.43737\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0081 - accuracy: 0.9949 - val_loss: 0.9881 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.43737\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0101 - accuracy: 0.9947 - val_loss: 1.0136 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.43737\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9925 - val_loss: 0.9871 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.43737\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0099 - accuracy: 0.9938 - val_loss: 0.9869 - val_accuracy: 0.8352\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.43737\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0128 - accuracy: 0.9915 - val_loss: 0.9918 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.43737\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9930 - val_loss: 1.0474 - val_accuracy: 0.8264\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.43737\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0101 - accuracy: 0.9948 - val_loss: 0.9866 - val_accuracy: 0.8297\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.43737\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0082 - accuracy: 0.9943 - val_loss: 1.0544 - val_accuracy: 0.8286\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.43737\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0059 - accuracy: 0.9970 - val_loss: 1.0176 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.43737\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0710 - accuracy: 0.9870 - val_loss: 0.6119 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.43737\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0267 - accuracy: 0.9918 - val_loss: 0.5750 - val_accuracy: 0.8681\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.43737\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0134 - accuracy: 0.9938 - val_loss: 0.5895 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.43737\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0093 - accuracy: 0.9953 - val_loss: 0.5844 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.43737\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9945 - val_loss: 0.6034 - val_accuracy: 0.8560\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.43737\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0069 - accuracy: 0.9953 - val_loss: 0.6211 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.43737\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9910 - val_loss: 0.6322 - val_accuracy: 0.8604\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.43737\n",
      "Epoch 83/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0082 - accuracy: 0.9960 - val_loss: 0.6444 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.43737\n",
      "Epoch 84/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.6571 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.43737\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0101 - accuracy: 0.9938 - val_loss: 0.6628 - val_accuracy: 0.8604\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.43737\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0072 - accuracy: 0.9962 - val_loss: 0.6736 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.43737\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0122 - accuracy: 0.9944 - val_loss: 0.6816 - val_accuracy: 0.8604\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.43737\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0081 - accuracy: 0.9943 - val_loss: 0.6908 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.43737\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0079 - accuracy: 0.9953 - val_loss: 0.6956 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.43737\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0110 - accuracy: 0.9922 - val_loss: 0.7033 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.43737\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0070 - accuracy: 0.9947 - val_loss: 0.7101 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.43737\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0092 - accuracy: 0.9929 - val_loss: 0.7224 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.43737\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0109 - accuracy: 0.9934 - val_loss: 0.7237 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.43737\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0075 - accuracy: 0.9952 - val_loss: 0.7327 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.43737\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0093 - accuracy: 0.9932 - val_loss: 0.7372 - val_accuracy: 0.8626\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.43737\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0103 - accuracy: 0.9938 - val_loss: 0.7438 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.43737\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0079 - accuracy: 0.9953 - val_loss: 0.7501 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.43737\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0082 - accuracy: 0.9952 - val_loss: 0.7606 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.43737\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0075 - accuracy: 0.9963 - val_loss: 0.7601 - val_accuracy: 0.8604\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.43737\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0078 - accuracy: 0.9957 - val_loss: 0.7649 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.43737\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0105 - accuracy: 0.9924 - val_loss: 0.7714 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.43737\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0080 - accuracy: 0.9955 - val_loss: 0.7775 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.43737\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0079 - accuracy: 0.9952 - val_loss: 0.7834 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.43737\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0071 - accuracy: 0.9962 - val_loss: 0.7886 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.43737\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0070 - accuracy: 0.9951 - val_loss: 0.7984 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.43737\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0072 - accuracy: 0.9947 - val_loss: 0.8047 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.43737\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0112 - accuracy: 0.9910 - val_loss: 0.8058 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.43737\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0118 - accuracy: 0.9941 - val_loss: 0.8117 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.43737\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0070 - accuracy: 0.9952 - val_loss: 0.8192 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.43737\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0096 - accuracy: 0.9951 - val_loss: 0.8226 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.43737\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0085 - accuracy: 0.9922 - val_loss: 0.8333 - val_accuracy: 0.8560\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.43737\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.8273 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.43737\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0066 - accuracy: 0.9965 - val_loss: 0.8335 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.43737\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0109 - accuracy: 0.9912 - val_loss: 0.8389 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.43737\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0093 - accuracy: 0.9934 - val_loss: 0.8461 - val_accuracy: 0.8560\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.43737\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0092 - accuracy: 0.9932 - val_loss: 0.8509 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.43737\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0077 - accuracy: 0.9954 - val_loss: 0.8565 - val_accuracy: 0.8582\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.43737\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0082 - accuracy: 0.9941 - val_loss: 0.8584 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.43737\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0080 - accuracy: 0.9948 - val_loss: 0.8620 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.43737\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0110 - accuracy: 0.9942 - val_loss: 0.8623 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.43737\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0088 - accuracy: 0.9943 - val_loss: 0.8721 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.43737\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0112 - accuracy: 0.9904 - val_loss: 0.8757 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.43737\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0081 - accuracy: 0.9950 - val_loss: 0.8810 - val_accuracy: 0.8527\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.43737\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0077 - accuracy: 0.9938 - val_loss: 0.8893 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.43737\n",
      "Epoch 125/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0094 - accuracy: 0.9949 - val_loss: 0.8927 - val_accuracy: 0.8527\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.43737\n",
      "Epoch 126/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0103 - accuracy: 0.9919 - val_loss: 0.8911 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.43737\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0111 - accuracy: 0.9935 - val_loss: 0.8900 - val_accuracy: 0.8560\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.43737\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0063 - accuracy: 0.9972 - val_loss: 0.8964 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.43737\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0089 - accuracy: 0.9945 - val_loss: 0.9014 - val_accuracy: 0.8560\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.43737\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0091 - accuracy: 0.9959 - val_loss: 0.9047 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.43737\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0081 - accuracy: 0.9931 - val_loss: 0.9120 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.43737\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0078 - accuracy: 0.9970 - val_loss: 0.9155 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.43737\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0082 - accuracy: 0.9954 - val_loss: 0.9169 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.43737\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0065 - accuracy: 0.9962 - val_loss: 0.9216 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.43737\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0082 - accuracy: 0.9961 - val_loss: 0.9272 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.43737\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9927 - val_loss: 0.9314 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.43737\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0091 - accuracy: 0.9941 - val_loss: 0.9365 - val_accuracy: 0.8560\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.43737\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 0.9929 - val_loss: 0.9349 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.43737\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 0.9955 - val_loss: 0.9363 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.43737\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0079 - accuracy: 0.9963 - val_loss: 0.9431 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.43737\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0094 - accuracy: 0.9935 - val_loss: 0.9470 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.43737\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0067 - accuracy: 0.9965 - val_loss: 0.9439 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.43737\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0060 - accuracy: 0.9976 - val_loss: 0.9577 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.43737\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0082 - accuracy: 0.9947 - val_loss: 0.9515 - val_accuracy: 0.8527\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.43737\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0089 - accuracy: 0.9931 - val_loss: 0.9558 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.43737\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0067 - accuracy: 0.9951 - val_loss: 0.9649 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.43737\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0080 - accuracy: 0.9949 - val_loss: 0.9750 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.43737\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0081 - accuracy: 0.9945 - val_loss: 0.9779 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.43737\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.9767 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.43737\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0077 - accuracy: 0.9942 - val_loss: 0.9878 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.43737\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0078 - accuracy: 0.9966 - val_loss: 0.9865 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.43737\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0056 - accuracy: 0.9962 - val_loss: 0.9838 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.43737\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.9895 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.43737\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0101 - accuracy: 0.9928 - val_loss: 0.9940 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.43737\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0089 - accuracy: 0.9952 - val_loss: 1.0043 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.43737\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0077 - accuracy: 0.9947 - val_loss: 0.9956 - val_accuracy: 0.8516\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.43737\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0073 - accuracy: 0.9950 - val_loss: 1.0024 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.43737\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0080 - accuracy: 0.9957 - val_loss: 0.9973 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.43737\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0089 - accuracy: 0.9933 - val_loss: 1.0080 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.43737\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0064 - accuracy: 0.9953 - val_loss: 1.0208 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.43737\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0070 - accuracy: 0.9962 - val_loss: 1.0159 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.43737\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0064 - accuracy: 0.9957 - val_loss: 1.0207 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.43737\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0085 - accuracy: 0.9953 - val_loss: 1.0281 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.43737\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0098 - accuracy: 0.9936 - val_loss: 1.0400 - val_accuracy: 0.8484\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.43737\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0082 - accuracy: 0.9958 - val_loss: 1.0382 - val_accuracy: 0.8451\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.43737\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0066 - accuracy: 0.9950 - val_loss: 1.0516 - val_accuracy: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00166: val_loss did not improve from 0.43737\n",
      "Epoch 167/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0074 - accuracy: 0.9972 - val_loss: 1.0341 - val_accuracy: 0.8385\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.43737\n",
      "Epoch 168/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0121 - accuracy: 0.9916 - val_loss: 1.0563 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.43737\n",
      "Epoch 169/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0075 - accuracy: 0.9963 - val_loss: 1.0483 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.43737\n",
      "Epoch 170/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0059 - accuracy: 0.9958 - val_loss: 1.0659 - val_accuracy: 0.8396\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.43737\n",
      "Epoch 171/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0076 - accuracy: 0.9961 - val_loss: 1.0609 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.43737\n",
      "Epoch 172/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 1.0744 - val_accuracy: 0.8407\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.43737\n",
      "Epoch 173/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0112 - accuracy: 0.9946 - val_loss: 1.0646 - val_accuracy: 0.8473\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.43737\n",
      "Epoch 174/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0074 - accuracy: 0.9954 - val_loss: 1.0795 - val_accuracy: 0.8407\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.43737\n",
      "Epoch 175/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0056 - accuracy: 0.9956 - val_loss: 1.0810 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.43737\n",
      "Epoch 176/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0080 - accuracy: 0.9948 - val_loss: 1.0813 - val_accuracy: 0.8451\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.43737\n",
      "Epoch 177/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0075 - accuracy: 0.9968 - val_loss: 1.0838 - val_accuracy: 0.8429\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.43737\n",
      "Epoch 178/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0069 - accuracy: 0.9952 - val_loss: 1.2519 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.43737\n",
      "Epoch 179/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0141 - accuracy: 0.9956 - val_loss: 0.8570 - val_accuracy: 0.7835\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.43737\n",
      "Epoch 180/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0121 - accuracy: 0.9932 - val_loss: 0.8500 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.43737\n",
      "Epoch 181/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 0.9941 - val_loss: 0.8258 - val_accuracy: 0.8154\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.43737\n",
      "Epoch 182/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0114 - accuracy: 0.9932 - val_loss: 0.8300 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.43737\n",
      "Epoch 183/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0070 - accuracy: 0.9945 - val_loss: 0.8338 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.43737\n",
      "Epoch 184/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0060 - accuracy: 0.9951 - val_loss: 0.8408 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.43737\n",
      "Epoch 185/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0103 - accuracy: 0.9919 - val_loss: 0.8469 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.43737\n",
      "Epoch 186/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0061 - accuracy: 0.9966 - val_loss: 0.8547 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.43737\n",
      "Epoch 187/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0051 - accuracy: 0.9962 - val_loss: 0.8619 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.43737\n",
      "Epoch 188/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0106 - accuracy: 0.9924 - val_loss: 0.8679 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.43737\n",
      "Epoch 189/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0081 - accuracy: 0.9946 - val_loss: 0.8744 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.43737\n",
      "Epoch 190/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0091 - accuracy: 0.9950 - val_loss: 0.8836 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.43737\n",
      "Epoch 191/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.8859 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.43737\n",
      "Epoch 192/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9942 - val_loss: 0.8930 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.43737\n",
      "Epoch 193/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0088 - accuracy: 0.9952 - val_loss: 0.9008 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.43737\n",
      "Epoch 194/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0096 - accuracy: 0.9934 - val_loss: 0.9053 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.43737\n",
      "Epoch 195/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0104 - accuracy: 0.9950 - val_loss: 0.9119 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.43737\n",
      "Epoch 196/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0075 - accuracy: 0.9944 - val_loss: 0.9170 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.43737\n",
      "Epoch 197/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9934 - val_loss: 0.9223 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.43737\n",
      "Epoch 198/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0071 - accuracy: 0.9968 - val_loss: 0.9270 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.43737\n",
      "Epoch 199/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0052 - accuracy: 0.9967 - val_loss: 0.9302 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.43737\n",
      "Epoch 200/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0078 - accuracy: 0.9948 - val_loss: 0.9350 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.43737\n",
      "Epoch 201/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 0.9961 - val_loss: 0.9419 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.43737\n",
      "Epoch 202/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9941 - val_loss: 0.9447 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.43737\n",
      "Epoch 203/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0077 - accuracy: 0.9941 - val_loss: 0.9496 - val_accuracy: 0.8154\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.43737\n",
      "Epoch 204/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0098 - accuracy: 0.9926 - val_loss: 0.9520 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.43737\n",
      "Epoch 205/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0092 - accuracy: 0.9947 - val_loss: 0.9559 - val_accuracy: 0.8154\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.43737\n",
      "Epoch 206/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0079 - accuracy: 0.9947 - val_loss: 0.9605 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.43737\n",
      "Epoch 207/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0083 - accuracy: 0.9956 - val_loss: 0.9611 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.43737\n",
      "Epoch 208/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0055 - accuracy: 0.9956 - val_loss: 0.9650 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.43737\n",
      "Epoch 209/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0098 - accuracy: 0.9947 - val_loss: 0.9684 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.43737\n",
      "Epoch 210/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 0.9943 - val_loss: 0.9699 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.43737\n",
      "Epoch 211/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0090 - accuracy: 0.9956 - val_loss: 0.9770 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.43737\n",
      "Epoch 212/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0086 - accuracy: 0.9964 - val_loss: 0.9822 - val_accuracy: 0.8143\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.43737\n",
      "Epoch 213/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9935 - val_loss: 0.9846 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.43737\n",
      "Epoch 214/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0081 - accuracy: 0.9948 - val_loss: 0.9852 - val_accuracy: 0.8121\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.43737\n",
      "Epoch 215/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0074 - accuracy: 0.9963 - val_loss: 0.9915 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.43737\n",
      "Epoch 216/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 0.9945 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.43737\n",
      "Epoch 217/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0092 - accuracy: 0.9950 - val_loss: 1.0002 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.43737\n",
      "Epoch 218/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0076 - accuracy: 0.9962 - val_loss: 1.0041 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.43737\n",
      "Epoch 219/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0080 - accuracy: 0.9948 - val_loss: 1.0016 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.43737\n",
      "Epoch 220/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0092 - accuracy: 0.9937 - val_loss: 1.0152 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.43737\n",
      "Epoch 221/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0078 - accuracy: 0.9952 - val_loss: 1.0127 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.43737\n",
      "Epoch 222/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0083 - accuracy: 0.9960 - val_loss: 1.0218 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.43737\n",
      "Epoch 223/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0086 - accuracy: 0.9922 - val_loss: 1.0250 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.43737\n",
      "Epoch 224/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0063 - accuracy: 0.9967 - val_loss: 1.0272 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.43737\n",
      "Epoch 225/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0082 - accuracy: 0.9930 - val_loss: 1.0374 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.43737\n",
      "Epoch 226/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0097 - accuracy: 0.9936 - val_loss: 1.0292 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.43737\n",
      "Epoch 227/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0084 - accuracy: 0.9946 - val_loss: 1.0368 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.43737\n",
      "Epoch 228/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0067 - accuracy: 0.9953 - val_loss: 1.0396 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.43737\n",
      "Epoch 229/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0092 - accuracy: 0.9935 - val_loss: 1.0485 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.43737\n",
      "Epoch 230/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 0.9963 - val_loss: 1.0586 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.43737\n",
      "Epoch 231/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0075 - accuracy: 0.9937 - val_loss: 1.0521 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.43737\n",
      "Epoch 232/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0093 - accuracy: 0.9948 - val_loss: 1.0575 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.43737\n",
      "Epoch 233/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0086 - accuracy: 0.9951 - val_loss: 1.0618 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.43737\n",
      "Epoch 234/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0054 - accuracy: 0.9954 - val_loss: 1.0718 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.43737\n",
      "Epoch 235/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0069 - accuracy: 0.9952 - val_loss: 1.0747 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.43737\n",
      "Epoch 236/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0080 - accuracy: 0.9916 - val_loss: 1.0913 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.43737\n",
      "Epoch 237/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0104 - accuracy: 0.9946 - val_loss: 1.0727 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.43737\n",
      "Epoch 238/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0077 - accuracy: 0.9960 - val_loss: 1.0747 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.43737\n",
      "Epoch 239/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0083 - accuracy: 0.9937 - val_loss: 1.0810 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.43737\n",
      "Epoch 240/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9925 - val_loss: 1.0912 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.43737\n",
      "Epoch 241/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0093 - accuracy: 0.9953 - val_loss: 1.0942 - val_accuracy: 0.8264\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.43737\n",
      "Epoch 242/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0111 - accuracy: 0.9940 - val_loss: 1.0973 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.43737\n",
      "Epoch 243/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0060 - accuracy: 0.9959 - val_loss: 1.1032 - val_accuracy: 0.8264\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.43737\n",
      "Epoch 244/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0077 - accuracy: 0.9965 - val_loss: 1.1062 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.43737\n",
      "Epoch 245/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0091 - accuracy: 0.9951 - val_loss: 1.1129 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.43737\n",
      "Epoch 246/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0074 - accuracy: 0.9943 - val_loss: 1.1386 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.43737\n",
      "Epoch 247/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0102 - accuracy: 0.9927 - val_loss: 1.1367 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.43737\n",
      "Epoch 248/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0060 - accuracy: 0.9946 - val_loss: 1.1262 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.43737\n",
      "Epoch 249/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0102 - accuracy: 0.9936 - val_loss: 1.1856 - val_accuracy: 0.8198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00249: val_loss did not improve from 0.43737\n",
      "Epoch 250/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0081 - accuracy: 0.9951 - val_loss: 1.1775 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.43737\n",
      "Epoch 251/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0092 - accuracy: 0.9943 - val_loss: 1.1775 - val_accuracy: 0.8275\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.43737\n",
      "Epoch 252/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0073 - accuracy: 0.9944 - val_loss: 1.1847 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.43737\n",
      "Epoch 253/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0063 - accuracy: 0.9970 - val_loss: 1.1772 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.43737\n",
      "Epoch 254/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0087 - accuracy: 0.9955 - val_loss: 1.1902 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.43737\n",
      "Epoch 255/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0072 - accuracy: 0.9935 - val_loss: 1.1900 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.43737\n",
      "Epoch 256/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0099 - accuracy: 0.9917 - val_loss: 1.1908 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.43737\n",
      "Epoch 257/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0072 - accuracy: 0.9949 - val_loss: 1.2094 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.43737\n",
      "Epoch 258/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0080 - accuracy: 0.9957 - val_loss: 1.2188 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.43737\n",
      "Epoch 259/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0102 - accuracy: 0.9927 - val_loss: 1.2232 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.43737\n",
      "Epoch 260/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0099 - accuracy: 0.9933 - val_loss: 1.2218 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.43737\n",
      "Epoch 261/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0065 - accuracy: 0.9956 - val_loss: 1.2250 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.43737\n",
      "Epoch 262/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0079 - accuracy: 0.9961 - val_loss: 1.2200 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.43737\n",
      "Epoch 263/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0065 - accuracy: 0.9961 - val_loss: 1.2210 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.43737\n",
      "Epoch 264/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0112 - accuracy: 0.9941 - val_loss: 1.2334 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.43737\n",
      "Epoch 265/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0104 - accuracy: 0.9923 - val_loss: 1.2405 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.43737\n",
      "Epoch 266/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0064 - accuracy: 0.9977 - val_loss: 1.2431 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.43737\n",
      "Epoch 267/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0081 - accuracy: 0.9950 - val_loss: 1.2439 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.43737\n",
      "Epoch 268/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0085 - accuracy: 0.9928 - val_loss: 1.2508 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.43737\n",
      "Epoch 269/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0086 - accuracy: 0.9961 - val_loss: 1.2547 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.43737\n",
      "Epoch 270/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0113 - accuracy: 0.9943 - val_loss: 1.2651 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.43737\n",
      "Epoch 271/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0072 - accuracy: 0.9957 - val_loss: 1.2737 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.43737\n",
      "Epoch 272/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0081 - accuracy: 0.9932 - val_loss: 1.2642 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.43737\n",
      "Epoch 273/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0103 - accuracy: 0.9937 - val_loss: 1.2861 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.43737\n",
      "Epoch 274/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0111 - accuracy: 0.9913 - val_loss: 1.2914 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.43737\n",
      "Epoch 275/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9925 - val_loss: 1.2878 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.43737\n",
      "Epoch 276/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0070 - accuracy: 0.9949 - val_loss: 1.2982 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.43737\n",
      "Epoch 277/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0083 - accuracy: 0.9946 - val_loss: 1.2808 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.43737\n",
      "Epoch 278/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0066 - accuracy: 0.9962 - val_loss: 1.2924 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.43737\n",
      "Epoch 279/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0070 - accuracy: 0.9945 - val_loss: 1.3079 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.43737\n",
      "Epoch 280/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0057 - accuracy: 0.9966 - val_loss: 1.2952 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.43737\n",
      "Epoch 281/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0084 - accuracy: 0.9955 - val_loss: 1.3053 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.43737\n",
      "Epoch 282/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0069 - accuracy: 0.9964 - val_loss: 1.3214 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.43737\n",
      "Epoch 283/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0071 - accuracy: 0.9966 - val_loss: 1.3122 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.43737\n",
      "Epoch 284/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0072 - accuracy: 0.9955 - val_loss: 1.3252 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.43737\n",
      "Epoch 285/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0073 - accuracy: 0.9933 - val_loss: 1.3138 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.43737\n",
      "Epoch 286/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0070 - accuracy: 0.9958 - val_loss: 1.3315 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.43737\n",
      "Epoch 287/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0068 - accuracy: 0.9944 - val_loss: 1.3535 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.43737\n",
      "Epoch 288/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0084 - accuracy: 0.9959 - val_loss: 1.3143 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.43737\n",
      "Epoch 289/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0131 - accuracy: 0.9915 - val_loss: 1.3377 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.43737\n",
      "Epoch 290/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0079 - accuracy: 0.9939 - val_loss: 1.3626 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.43737\n",
      "Epoch 291/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0070 - accuracy: 0.9973 - val_loss: 1.3584 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.43737\n",
      "Epoch 292/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0101 - accuracy: 0.9928 - val_loss: 1.3680 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.43737\n",
      "Epoch 293/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0072 - accuracy: 0.9953 - val_loss: 1.3688 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.43737\n",
      "Epoch 294/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0125 - accuracy: 0.9924 - val_loss: 1.3830 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.43737\n",
      "Epoch 295/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0077 - accuracy: 0.9941 - val_loss: 1.3785 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.43737\n",
      "Epoch 296/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0072 - accuracy: 0.9950 - val_loss: 1.3934 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.43737\n",
      "Epoch 297/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0089 - accuracy: 0.9938 - val_loss: 1.4014 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.43737\n",
      "Epoch 298/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 0.9943 - val_loss: 1.4097 - val_accuracy: 0.8242\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.43737\n",
      "Epoch 299/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0067 - accuracy: 0.9943 - val_loss: 1.3951 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.43737\n",
      "Epoch 300/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0090 - accuracy: 0.9946 - val_loss: 1.4205 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.43737\n",
      "Epoch 301/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0085 - accuracy: 0.9960 - val_loss: 1.4066 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.43737\n",
      "Epoch 302/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0085 - accuracy: 0.9938 - val_loss: 1.4118 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.43737\n",
      "Epoch 303/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0092 - accuracy: 0.9958 - val_loss: 1.4285 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.43737\n",
      "Epoch 304/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0100 - accuracy: 0.9933 - val_loss: 1.4197 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.43737\n",
      "Epoch 305/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0081 - accuracy: 0.9933 - val_loss: 1.4337 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.43737\n"
     ]
    }
   ],
   "source": [
    "# LSTM 설정\n",
    "model = Sequential()\n",
    "# Embedding( 불러온 단어의 총 개수, 기사당 단어의 수) : 데이터 전처리 과정을 통해, \n",
    "# 입력된 값을 받아 다음 층이 알아들을 수 있는 형태로 변환하는 역할\n",
    "model.add(Embedding( word_size, max_len ) ) \n",
    "model.add(LSTM(49, activation='tanh'))\n",
    "model.add(Dense(5, activation='softmax')) # 다중분류\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile( loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "\n",
    "if os.path.exists( f'/model/LSTM_0514/num_words_{num_word}/'):\n",
    "    os.mkdir(f'/model/LSTM_0514/num_words_{num_word}/' )\n",
    "    \n",
    "model_path = 'model/LSTM_0514/{epoch}-{val_loss}-{val_accuracy}.h5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', \n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',patience=300)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_x, \n",
    "                                                     y_encoded, \n",
    "                                                     test_size=0.3, \n",
    "                                                     stratify=y_encoded)\n",
    "\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit( X_train, y_train, validation_split=0.3,\n",
    "                    epochs=1500,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:26:43.630235Z",
     "start_time": "2021-05-13T16:26:43.626245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 43)          645000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 49)                18228     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 250       \n",
      "=================================================================\n",
      "Total params: 663,478\n",
      "Trainable params: 663,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:26:43.820308Z",
     "start_time": "2021-05-13T16:26:43.631233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트셋으로 실험 결과의 오차값을 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋으로 측정한 정확도의 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# x값을 지정하고 그래프로 확인\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.title('[Figure] LSTM 모델의 결과')\n",
    "plt.plot( x_len, y_vloss, 'o', c='red', markersize=3, label='TestSet Loss')\n",
    "plt.plot(x_len, y_acc, 'o', c='blue', markersize=3, label='TrainSet Acuraccy')\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.savefig(f'LSTM_num_words_{num_word}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:26:44.312617Z",
     "start_time": "2021-05-13T16:26:43.821312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 3ms/step - loss: 0.4358 - accuracy: 0.9436\n",
      "0.9436016082763672\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 1.5133 - accuracy: 0.8069\n",
      "0.8069230914115906\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 정확도\n",
    "print(model.evaluate(X_train,y_train)[1])\n",
    "\n",
    "# 테스트 데이터 정확도\n",
    "print(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:30:12.025444Z",
     "start_time": "2021-05-13T16:26:44.313614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "67/67 [==============================] - 3s 15ms/step - loss: 1.3977 - accuracy: 0.3361 - val_loss: 1.1004 - val_accuracy: 0.5989\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.10039, saving model to model/CNN+LSTM_0514\\1-1.1003903150558472-0.598901093006134.h5\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.8741 - accuracy: 0.6819 - val_loss: 0.5681 - val_accuracy: 0.7813\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.10039 to 0.56812, saving model to model/CNN+LSTM_0514\\2-0.5681153535842896-0.7813186645507812.h5\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.3914 - accuracy: 0.8461 - val_loss: 0.5005 - val_accuracy: 0.7967\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56812 to 0.50051, saving model to model/CNN+LSTM_0514\\3-0.5005072355270386-0.7967032790184021.h5\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.2259 - accuracy: 0.9269 - val_loss: 0.5092 - val_accuracy: 0.8132\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.50051\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.1621 - accuracy: 0.9396 - val_loss: 0.4995 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50051 to 0.49947, saving model to model/CNN+LSTM_0514\\5-0.4994731843471527-0.8164834976196289.h5\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.1246 - accuracy: 0.9553 - val_loss: 0.5425 - val_accuracy: 0.7989\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49947\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.1193 - accuracy: 0.9518 - val_loss: 0.5541 - val_accuracy: 0.8099\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49947\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.1152 - accuracy: 0.9475 - val_loss: 0.6189 - val_accuracy: 0.7967\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49947\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0883 - accuracy: 0.9662 - val_loss: 0.6918 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49947\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0885 - accuracy: 0.9651 - val_loss: 0.5789 - val_accuracy: 0.8154\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49947\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0855 - accuracy: 0.9609 - val_loss: 0.6238 - val_accuracy: 0.7934\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49947\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0837 - accuracy: 0.9637 - val_loss: 0.6324 - val_accuracy: 0.7967\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49947\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0737 - accuracy: 0.9678 - val_loss: 0.7007 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49947\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0751 - accuracy: 0.9626 - val_loss: 0.6852 - val_accuracy: 0.7901\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49947\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0865 - accuracy: 0.9585 - val_loss: 0.7056 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49947\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0747 - accuracy: 0.9660 - val_loss: 0.7312 - val_accuracy: 0.7824\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.49947\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0729 - accuracy: 0.9619 - val_loss: 0.7233 - val_accuracy: 0.7901\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49947\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0796 - accuracy: 0.9664 - val_loss: 0.7944 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49947\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0720 - accuracy: 0.9650 - val_loss: 0.7168 - val_accuracy: 0.7901\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49947\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0712 - accuracy: 0.9646 - val_loss: 0.7246 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49947\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0696 - accuracy: 0.9661 - val_loss: 0.7717 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.49947\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0649 - accuracy: 0.9638 - val_loss: 0.7942 - val_accuracy: 0.7868\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.49947\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0735 - accuracy: 0.9604 - val_loss: 0.7626 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49947\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0651 - accuracy: 0.9722 - val_loss: 0.7958 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.49947\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0607 - accuracy: 0.9684 - val_loss: 0.8117 - val_accuracy: 0.7835\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.49947\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0691 - accuracy: 0.9686 - val_loss: 0.7975 - val_accuracy: 0.7912\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49947\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0542 - accuracy: 0.9731 - val_loss: 0.7865 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49947\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0586 - accuracy: 0.9693 - val_loss: 0.8050 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.49947\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0647 - accuracy: 0.9667 - val_loss: 0.8513 - val_accuracy: 0.7802\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.49947\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0613 - accuracy: 0.9703 - val_loss: 0.8295 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.49947\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0567 - accuracy: 0.9700 - val_loss: 0.8132 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.49947\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0591 - accuracy: 0.9712 - val_loss: 0.8352 - val_accuracy: 0.7780\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.49947\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0531 - accuracy: 0.9725 - val_loss: 0.8371 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.49947\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0599 - accuracy: 0.9748 - val_loss: 0.8667 - val_accuracy: 0.7802\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.49947\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0522 - accuracy: 0.9741 - val_loss: 0.8579 - val_accuracy: 0.7780\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.49947\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.0622 - accuracy: 0.9635 - val_loss: 0.8429 - val_accuracy: 0.7813\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.49947\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0665 - accuracy: 0.9677 - val_loss: 0.8801 - val_accuracy: 0.7813\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.49947\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0542 - accuracy: 0.9744 - val_loss: 0.8600 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.49947\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0581 - accuracy: 0.9697 - val_loss: 0.8362 - val_accuracy: 0.7835\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.49947\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0649 - accuracy: 0.9618 - val_loss: 0.8642 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.49947\n",
      "Epoch 41/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0488 - accuracy: 0.9724 - val_loss: 0.8664 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.49947\n",
      "Epoch 42/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0548 - accuracy: 0.9730 - val_loss: 0.8675 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.49947\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0564 - accuracy: 0.9700 - val_loss: 0.9413 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.49947\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0642 - accuracy: 0.9684 - val_loss: 0.9542 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.49947\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0551 - accuracy: 0.9725 - val_loss: 0.9155 - val_accuracy: 0.7791\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.49947\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0625 - accuracy: 0.9641 - val_loss: 0.8989 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.49947\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0619 - accuracy: 0.9630 - val_loss: 0.9397 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.49947\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0560 - accuracy: 0.9679 - val_loss: 0.9244 - val_accuracy: 0.7813\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.49947\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0554 - accuracy: 0.9720 - val_loss: 0.9388 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.49947\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0521 - accuracy: 0.9744 - val_loss: 0.9285 - val_accuracy: 0.7802\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.49947\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0606 - accuracy: 0.9689 - val_loss: 1.0030 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.49947\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0643 - accuracy: 0.9666 - val_loss: 0.9470 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.49947\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0495 - accuracy: 0.9745 - val_loss: 0.9620 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.49947\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0554 - accuracy: 0.9693 - val_loss: 0.9518 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.49947\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0606 - accuracy: 0.9698 - val_loss: 0.9729 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.49947\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0621 - accuracy: 0.9652 - val_loss: 0.9745 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.49947\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0590 - accuracy: 0.9656 - val_loss: 0.9664 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.49947\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0576 - accuracy: 0.9697 - val_loss: 0.9610 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.49947\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0534 - accuracy: 0.9627 - val_loss: 0.9862 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.49947\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0579 - accuracy: 0.9658 - val_loss: 1.0195 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.49947\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0567 - accuracy: 0.9720 - val_loss: 0.9884 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.49947\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0599 - accuracy: 0.9680 - val_loss: 0.9618 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.49947\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0617 - accuracy: 0.9617 - val_loss: 0.9886 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.49947\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0543 - accuracy: 0.9755 - val_loss: 0.9955 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.49947\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0505 - accuracy: 0.9724 - val_loss: 0.9827 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.49947\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0564 - accuracy: 0.9674 - val_loss: 1.0057 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.49947\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0529 - accuracy: 0.9705 - val_loss: 1.0208 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.49947\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0615 - accuracy: 0.9686 - val_loss: 1.0388 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.49947\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0459 - accuracy: 0.9751 - val_loss: 1.0225 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.49947\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0529 - accuracy: 0.9727 - val_loss: 1.0219 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.49947\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0578 - accuracy: 0.9671 - val_loss: 1.0162 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.49947\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0547 - accuracy: 0.9712 - val_loss: 1.0303 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.49947\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0483 - accuracy: 0.9724 - val_loss: 1.0277 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.49947\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0493 - accuracy: 0.9723 - val_loss: 1.0048 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.49947\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0575 - accuracy: 0.9645 - val_loss: 1.0398 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.49947\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0608 - accuracy: 0.9665 - val_loss: 1.0341 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.49947\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0591 - accuracy: 0.9695 - val_loss: 1.0370 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.49947\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0590 - accuracy: 0.9685 - val_loss: 1.0234 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.49947\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0540 - accuracy: 0.9648 - val_loss: 1.0517 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.49947\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0524 - accuracy: 0.9699 - val_loss: 1.0496 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.49947\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0535 - accuracy: 0.9732 - val_loss: 1.0418 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.49947\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0556 - accuracy: 0.9707 - val_loss: 1.0655 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.49947\n",
      "Epoch 83/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0564 - accuracy: 0.9720 - val_loss: 1.0627 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.49947\n",
      "Epoch 84/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0522 - accuracy: 0.9716 - val_loss: 1.0495 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.49947\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0552 - accuracy: 0.9643 - val_loss: 1.0665 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.49947\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0548 - accuracy: 0.9682 - val_loss: 1.0766 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.49947\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0580 - accuracy: 0.9664 - val_loss: 1.0702 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.49947\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0527 - accuracy: 0.9717 - val_loss: 1.0407 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.49947\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0500 - accuracy: 0.9751 - val_loss: 1.0850 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.49947\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0616 - accuracy: 0.9636 - val_loss: 1.0591 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.49947\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0510 - accuracy: 0.9745 - val_loss: 1.0977 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.49947\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0610 - accuracy: 0.9637 - val_loss: 1.0771 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.49947\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0523 - accuracy: 0.9675 - val_loss: 1.0901 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.49947\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0603 - accuracy: 0.9632 - val_loss: 1.0744 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.49947\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0520 - accuracy: 0.9732 - val_loss: 1.1300 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.49947\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0593 - accuracy: 0.9649 - val_loss: 1.1354 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.49947\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0620 - accuracy: 0.9647 - val_loss: 1.0978 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.49947\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0585 - accuracy: 0.9665 - val_loss: 1.1331 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.49947\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0540 - accuracy: 0.9675 - val_loss: 1.1250 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.49947\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0592 - accuracy: 0.9656 - val_loss: 1.1277 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.49947\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0564 - accuracy: 0.9684 - val_loss: 1.1142 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.49947\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0542 - accuracy: 0.9720 - val_loss: 1.1162 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.49947\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0513 - accuracy: 0.9728 - val_loss: 1.1093 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.49947\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0607 - accuracy: 0.9662 - val_loss: 1.1138 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.49947\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0567 - accuracy: 0.9680 - val_loss: 1.1379 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.49947\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0543 - accuracy: 0.9641 - val_loss: 1.1280 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.49947\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0567 - accuracy: 0.9696 - val_loss: 1.1389 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.49947\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0468 - accuracy: 0.9754 - val_loss: 1.1191 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.49947\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0605 - accuracy: 0.9688 - val_loss: 1.1202 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.49947\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0567 - accuracy: 0.9661 - val_loss: 1.1107 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.49947\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0597 - accuracy: 0.9637 - val_loss: 1.1354 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.49947\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0532 - accuracy: 0.9696 - val_loss: 1.1375 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.49947\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0523 - accuracy: 0.9739 - val_loss: 1.1638 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.49947\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0574 - accuracy: 0.9678 - val_loss: 1.1296 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.49947\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0579 - accuracy: 0.9661 - val_loss: 1.0977 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.49947\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0541 - accuracy: 0.9699 - val_loss: 1.1610 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.49947\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0523 - accuracy: 0.9708 - val_loss: 1.1801 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.49947\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0483 - accuracy: 0.9795 - val_loss: 1.1360 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.49947\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0541 - accuracy: 0.9676 - val_loss: 1.1560 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.49947\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0518 - accuracy: 0.9708 - val_loss: 1.1802 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.49947\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0536 - accuracy: 0.9663 - val_loss: 1.1451 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.49947\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0547 - accuracy: 0.9714 - val_loss: 1.1485 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.49947\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0592 - accuracy: 0.9677 - val_loss: 1.1509 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.49947\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0570 - accuracy: 0.9656 - val_loss: 1.1710 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.49947\n",
      "Epoch 125/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0542 - accuracy: 0.9692 - val_loss: 1.1221 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.49947\n",
      "Epoch 126/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0501 - accuracy: 0.9713 - val_loss: 1.1609 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.49947\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0510 - accuracy: 0.9736 - val_loss: 1.1383 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.49947\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0450 - accuracy: 0.9744 - val_loss: 1.1340 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.49947\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0564 - accuracy: 0.9646 - val_loss: 1.1683 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.49947\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0518 - accuracy: 0.9730 - val_loss: 1.1953 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.49947\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0485 - accuracy: 0.9723 - val_loss: 1.1680 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.49947\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0513 - accuracy: 0.9723 - val_loss: 1.1765 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.49947\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0485 - accuracy: 0.9717 - val_loss: 1.1864 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.49947\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0619 - accuracy: 0.9640 - val_loss: 1.1823 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.49947\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0551 - accuracy: 0.9676 - val_loss: 1.1915 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.49947\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0511 - accuracy: 0.9724 - val_loss: 1.1878 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.49947\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0523 - accuracy: 0.9728 - val_loss: 1.1939 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.49947\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0567 - accuracy: 0.9663 - val_loss: 1.2101 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.49947\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0518 - accuracy: 0.9718 - val_loss: 1.1981 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.49947\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0530 - accuracy: 0.9744 - val_loss: 1.2010 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.49947\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0552 - accuracy: 0.9683 - val_loss: 1.2036 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.49947\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0523 - accuracy: 0.9671 - val_loss: 1.2229 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.49947\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0517 - accuracy: 0.9731 - val_loss: 1.2030 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.49947\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0545 - accuracy: 0.9686 - val_loss: 1.1834 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.49947\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0487 - accuracy: 0.9712 - val_loss: 1.1855 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.49947\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0556 - accuracy: 0.9630 - val_loss: 1.2350 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.49947\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0484 - accuracy: 0.9732 - val_loss: 1.2302 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.49947\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0538 - accuracy: 0.9667 - val_loss: 1.2274 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.49947\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0564 - accuracy: 0.9650 - val_loss: 1.2381 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.49947\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0556 - accuracy: 0.9684 - val_loss: 1.2150 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.49947\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0568 - accuracy: 0.9625 - val_loss: 1.2244 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.49947\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0583 - accuracy: 0.9713 - val_loss: 1.2294 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.49947\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0566 - accuracy: 0.9655 - val_loss: 1.2453 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.49947\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0558 - accuracy: 0.9681 - val_loss: 1.2563 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.49947\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0591 - accuracy: 0.9648 - val_loss: 1.2085 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.49947\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0494 - accuracy: 0.9727 - val_loss: 1.2350 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.49947\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0591 - accuracy: 0.9630 - val_loss: 1.2168 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.49947\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0531 - accuracy: 0.9701 - val_loss: 1.2065 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.49947\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0568 - accuracy: 0.9658 - val_loss: 1.1987 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.49947\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0540 - accuracy: 0.9722 - val_loss: 1.2432 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.49947\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0516 - accuracy: 0.9725 - val_loss: 1.2142 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.49947\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0442 - accuracy: 0.9767 - val_loss: 1.2389 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.49947\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0516 - accuracy: 0.9694 - val_loss: 1.2346 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.49947\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0564 - accuracy: 0.9701 - val_loss: 1.2320 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.49947\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0507 - accuracy: 0.9718 - val_loss: 1.2761 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.49947\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0503 - accuracy: 0.9706 - val_loss: 1.2407 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.49947\n",
      "Epoch 167/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0476 - accuracy: 0.9738 - val_loss: 1.2500 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.49947\n",
      "Epoch 168/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0541 - accuracy: 0.9689 - val_loss: 1.2414 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.49947\n",
      "Epoch 169/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0538 - accuracy: 0.9725 - val_loss: 1.2430 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.49947\n",
      "Epoch 170/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0501 - accuracy: 0.9720 - val_loss: 1.3080 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.49947\n",
      "Epoch 171/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0524 - accuracy: 0.9692 - val_loss: 1.2577 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.49947\n",
      "Epoch 172/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0468 - accuracy: 0.9716 - val_loss: 1.2612 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.49947\n",
      "Epoch 173/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0531 - accuracy: 0.9670 - val_loss: 1.2595 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.49947\n",
      "Epoch 174/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0559 - accuracy: 0.9700 - val_loss: 1.2341 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.49947\n",
      "Epoch 175/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0513 - accuracy: 0.9742 - val_loss: 1.3990 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.49947\n",
      "Epoch 176/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0550 - accuracy: 0.9690 - val_loss: 1.2885 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.49947\n",
      "Epoch 177/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0651 - accuracy: 0.9592 - val_loss: 1.3144 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.49947\n",
      "Epoch 178/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0533 - accuracy: 0.9736 - val_loss: 1.2824 - val_accuracy: 0.7659\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.49947\n",
      "Epoch 179/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0498 - accuracy: 0.9734 - val_loss: 1.2792 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.49947\n",
      "Epoch 180/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0498 - accuracy: 0.9690 - val_loss: 1.2668 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.49947\n",
      "Epoch 181/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0567 - accuracy: 0.9661 - val_loss: 1.2709 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.49947\n",
      "Epoch 182/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0530 - accuracy: 0.9725 - val_loss: 1.2760 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.49947\n",
      "Epoch 183/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0540 - accuracy: 0.9680 - val_loss: 1.2674 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.49947\n",
      "Epoch 184/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0573 - accuracy: 0.9665 - val_loss: 1.2908 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.49947\n",
      "Epoch 185/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0479 - accuracy: 0.9683 - val_loss: 1.2957 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.49947\n",
      "Epoch 186/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0534 - accuracy: 0.9658 - val_loss: 1.2880 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.49947\n",
      "Epoch 187/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0471 - accuracy: 0.9756 - val_loss: 1.2885 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.49947\n",
      "Epoch 188/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0556 - accuracy: 0.9645 - val_loss: 1.2991 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.49947\n",
      "Epoch 189/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0475 - accuracy: 0.9727 - val_loss: 1.3005 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.49947\n",
      "Epoch 190/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0569 - accuracy: 0.9697 - val_loss: 1.2922 - val_accuracy: 0.7714\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.49947\n",
      "Epoch 191/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0551 - accuracy: 0.9663 - val_loss: 1.3067 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.49947\n",
      "Epoch 192/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0544 - accuracy: 0.9719 - val_loss: 1.2929 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.49947\n",
      "Epoch 193/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0470 - accuracy: 0.9736 - val_loss: 1.3009 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.49947\n",
      "Epoch 194/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0585 - accuracy: 0.9655 - val_loss: 1.3414 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.49947\n",
      "Epoch 195/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0481 - accuracy: 0.9751 - val_loss: 1.3166 - val_accuracy: 0.7769\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.49947\n",
      "Epoch 196/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0534 - accuracy: 0.9681 - val_loss: 1.3123 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.49947\n",
      "Epoch 197/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0583 - accuracy: 0.9645 - val_loss: 1.3046 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.49947\n",
      "Epoch 198/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0533 - accuracy: 0.9707 - val_loss: 1.2937 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.49947\n",
      "Epoch 199/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0508 - accuracy: 0.9684 - val_loss: 1.3125 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.49947\n",
      "Epoch 200/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0576 - accuracy: 0.9663 - val_loss: 1.3193 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.49947\n",
      "Epoch 201/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0527 - accuracy: 0.9677 - val_loss: 1.2982 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.49947\n",
      "Epoch 202/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0545 - accuracy: 0.9692 - val_loss: 1.3352 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.49947\n",
      "Epoch 203/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0633 - accuracy: 0.9597 - val_loss: 1.3046 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.49947\n",
      "Epoch 204/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0553 - accuracy: 0.9636 - val_loss: 1.3000 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.49947\n",
      "Epoch 205/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0473 - accuracy: 0.9739 - val_loss: 1.3117 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.49947\n",
      "Epoch 206/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0543 - accuracy: 0.9618 - val_loss: 1.3158 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.49947\n",
      "Epoch 207/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0539 - accuracy: 0.9652 - val_loss: 1.3215 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.49947\n",
      "Epoch 208/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0591 - accuracy: 0.9654 - val_loss: 1.3327 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.49947\n",
      "Epoch 209/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 13ms/step - loss: 0.0541 - accuracy: 0.9664 - val_loss: 1.3337 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.49947\n",
      "Epoch 210/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0503 - accuracy: 0.9728 - val_loss: 1.3378 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.49947\n",
      "Epoch 211/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0630 - accuracy: 0.9591 - val_loss: 1.3429 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.49947\n",
      "Epoch 212/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0592 - accuracy: 0.9652 - val_loss: 1.3354 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.49947\n",
      "Epoch 213/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0481 - accuracy: 0.9753 - val_loss: 1.3488 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.49947\n",
      "Epoch 214/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0594 - accuracy: 0.9688 - val_loss: 1.3306 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.49947\n",
      "Epoch 215/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0551 - accuracy: 0.9697 - val_loss: 1.3546 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.49947\n",
      "Epoch 216/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0527 - accuracy: 0.9718 - val_loss: 1.3602 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.49947\n",
      "Epoch 217/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0527 - accuracy: 0.9667 - val_loss: 1.3661 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.49947\n",
      "Epoch 218/1500\n",
      "67/67 [==============================] - 1s 14ms/step - loss: 0.0532 - accuracy: 0.9649 - val_loss: 1.3692 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.49947\n",
      "Epoch 219/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0558 - accuracy: 0.9675 - val_loss: 1.3527 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.49947\n",
      "Epoch 220/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0482 - accuracy: 0.9750 - val_loss: 1.3590 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.49947\n",
      "Epoch 221/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0483 - accuracy: 0.9704 - val_loss: 1.3721 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.49947\n",
      "Epoch 222/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0516 - accuracy: 0.9724 - val_loss: 1.3620 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.49947\n",
      "Epoch 223/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0584 - accuracy: 0.9664 - val_loss: 1.3653 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.49947\n",
      "Epoch 224/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0526 - accuracy: 0.9675 - val_loss: 1.3583 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.49947\n",
      "Epoch 225/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0479 - accuracy: 0.9744 - val_loss: 1.3671 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.49947\n",
      "Epoch 226/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0559 - accuracy: 0.9691 - val_loss: 1.3671 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.49947\n",
      "Epoch 227/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0520 - accuracy: 0.9713 - val_loss: 1.4078 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.49947\n",
      "Epoch 228/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0563 - accuracy: 0.9708 - val_loss: 1.3675 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.49947\n",
      "Epoch 229/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0554 - accuracy: 0.9681 - val_loss: 1.3599 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.49947\n",
      "Epoch 230/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0489 - accuracy: 0.9736 - val_loss: 1.4060 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.49947\n",
      "Epoch 231/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0519 - accuracy: 0.9693 - val_loss: 1.3875 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.49947\n",
      "Epoch 232/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0446 - accuracy: 0.9782 - val_loss: 1.4015 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.49947\n",
      "Epoch 233/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0542 - accuracy: 0.9709 - val_loss: 1.3859 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.49947\n",
      "Epoch 234/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0520 - accuracy: 0.9717 - val_loss: 1.4087 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.49947\n",
      "Epoch 235/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0597 - accuracy: 0.9662 - val_loss: 1.3990 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.49947\n",
      "Epoch 236/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0522 - accuracy: 0.9687 - val_loss: 1.3892 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.49947\n",
      "Epoch 237/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0575 - accuracy: 0.9670 - val_loss: 1.3811 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.49947\n",
      "Epoch 238/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0473 - accuracy: 0.9723 - val_loss: 1.4044 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.49947\n",
      "Epoch 239/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0502 - accuracy: 0.9698 - val_loss: 1.4084 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.49947\n",
      "Epoch 240/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0565 - accuracy: 0.9610 - val_loss: 1.4179 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.49947\n",
      "Epoch 241/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0545 - accuracy: 0.9680 - val_loss: 1.4130 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.49947\n",
      "Epoch 242/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0592 - accuracy: 0.9651 - val_loss: 1.4187 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.49947\n",
      "Epoch 243/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0462 - accuracy: 0.9747 - val_loss: 1.4201 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.49947\n",
      "Epoch 244/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0541 - accuracy: 0.9726 - val_loss: 1.4285 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.49947\n",
      "Epoch 245/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0549 - accuracy: 0.9692 - val_loss: 1.4352 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.49947\n",
      "Epoch 246/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0501 - accuracy: 0.9717 - val_loss: 1.4163 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.49947\n",
      "Epoch 247/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0526 - accuracy: 0.9679 - val_loss: 1.4472 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.49947\n",
      "Epoch 248/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0594 - accuracy: 0.9687 - val_loss: 1.4242 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.49947\n",
      "Epoch 249/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0502 - accuracy: 0.9722 - val_loss: 1.4301 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.49947\n",
      "Epoch 250/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0585 - accuracy: 0.9619 - val_loss: 1.4460 - val_accuracy: 0.7571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00250: val_loss did not improve from 0.49947\n",
      "Epoch 251/1500\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 0.0505 - accuracy: 0.9735 - val_loss: 1.4440 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.49947\n",
      "Epoch 252/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0595 - accuracy: 0.9682 - val_loss: 1.4406 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.49947\n",
      "Epoch 253/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0515 - accuracy: 0.9725 - val_loss: 1.4616 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.49947\n",
      "Epoch 254/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0509 - accuracy: 0.9732 - val_loss: 1.4617 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.49947\n",
      "Epoch 255/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0539 - accuracy: 0.9690 - val_loss: 1.4630 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.49947\n",
      "Epoch 256/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0461 - accuracy: 0.9731 - val_loss: 1.4648 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.49947\n",
      "Epoch 257/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0572 - accuracy: 0.9658 - val_loss: 1.4629 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.49947\n",
      "Epoch 258/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0504 - accuracy: 0.9708 - val_loss: 1.4779 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.49947\n",
      "Epoch 259/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0543 - accuracy: 0.9706 - val_loss: 1.4816 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.49947\n",
      "Epoch 260/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0572 - accuracy: 0.9642 - val_loss: 1.4890 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.49947\n",
      "Epoch 261/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0575 - accuracy: 0.9645 - val_loss: 1.4963 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.49947\n",
      "Epoch 262/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0483 - accuracy: 0.9757 - val_loss: 1.4800 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.49947\n",
      "Epoch 263/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0563 - accuracy: 0.9679 - val_loss: 1.4911 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.49947\n",
      "Epoch 264/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0516 - accuracy: 0.9667 - val_loss: 1.5064 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.49947\n",
      "Epoch 265/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0525 - accuracy: 0.9720 - val_loss: 1.4923 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.49947\n",
      "Epoch 266/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0502 - accuracy: 0.9717 - val_loss: 1.5095 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.49947\n",
      "Epoch 267/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0572 - accuracy: 0.9679 - val_loss: 1.4989 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.49947\n",
      "Epoch 268/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0551 - accuracy: 0.9698 - val_loss: 1.4995 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.49947\n",
      "Epoch 269/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0582 - accuracy: 0.9649 - val_loss: 1.5112 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.49947\n",
      "Epoch 270/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0458 - accuracy: 0.9748 - val_loss: 1.4952 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.49947\n",
      "Epoch 271/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0549 - accuracy: 0.9711 - val_loss: 1.4882 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.49947\n",
      "Epoch 272/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0552 - accuracy: 0.9698 - val_loss: 1.4846 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.49947\n",
      "Epoch 273/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0547 - accuracy: 0.9676 - val_loss: 1.5130 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.49947\n",
      "Epoch 274/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0521 - accuracy: 0.9693 - val_loss: 1.5173 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.49947\n",
      "Epoch 275/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0563 - accuracy: 0.9641 - val_loss: 1.5226 - val_accuracy: 0.7516\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.49947\n",
      "Epoch 276/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0582 - accuracy: 0.9638 - val_loss: 1.5213 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.49947\n",
      "Epoch 277/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0534 - accuracy: 0.9711 - val_loss: 1.5007 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.49947\n",
      "Epoch 278/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0519 - accuracy: 0.9709 - val_loss: 1.5224 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.49947\n",
      "Epoch 279/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0475 - accuracy: 0.9725 - val_loss: 1.5276 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.49947\n",
      "Epoch 280/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0509 - accuracy: 0.9750 - val_loss: 1.5288 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.49947\n",
      "Epoch 281/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0525 - accuracy: 0.9702 - val_loss: 1.5290 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.49947\n",
      "Epoch 282/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0482 - accuracy: 0.9738 - val_loss: 1.5255 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.49947\n",
      "Epoch 283/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0440 - accuracy: 0.9762 - val_loss: 1.5349 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.49947\n",
      "Epoch 284/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0553 - accuracy: 0.9661 - val_loss: 1.5372 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.49947\n",
      "Epoch 285/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0512 - accuracy: 0.9729 - val_loss: 1.5270 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.49947\n",
      "Epoch 286/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0488 - accuracy: 0.9741 - val_loss: 1.5196 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.49947\n",
      "Epoch 287/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0637 - accuracy: 0.9617 - val_loss: 1.5593 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.49947\n",
      "Epoch 288/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0508 - accuracy: 0.9690 - val_loss: 1.5397 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.49947\n",
      "Epoch 289/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0464 - accuracy: 0.9760 - val_loss: 1.5077 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.49947\n",
      "Epoch 290/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0568 - accuracy: 0.9674 - val_loss: 1.5168 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.49947\n",
      "Epoch 291/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0575 - accuracy: 0.9669 - val_loss: 1.5096 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.49947\n",
      "Epoch 292/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0463 - accuracy: 0.9722 - val_loss: 1.5129 - val_accuracy: 0.7571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00292: val_loss did not improve from 0.49947\n",
      "Epoch 293/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0536 - accuracy: 0.9707 - val_loss: 1.5116 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.49947\n",
      "Epoch 294/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0542 - accuracy: 0.9681 - val_loss: 1.5206 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.49947\n",
      "Epoch 295/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0609 - accuracy: 0.9614 - val_loss: 1.4774 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.49947\n",
      "Epoch 296/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0619 - accuracy: 0.9644 - val_loss: 1.5430 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.49947\n",
      "Epoch 297/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0504 - accuracy: 0.9732 - val_loss: 1.5353 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.49947\n",
      "Epoch 298/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0612 - accuracy: 0.9633 - val_loss: 1.3529 - val_accuracy: 0.7516\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.49947\n",
      "Epoch 299/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0490 - accuracy: 0.9724 - val_loss: 1.3880 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.49947\n",
      "Epoch 300/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0465 - accuracy: 0.9751 - val_loss: 1.5128 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.49947\n",
      "Epoch 301/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0535 - accuracy: 0.9691 - val_loss: 1.4506 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.49947\n",
      "Epoch 302/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0449 - accuracy: 0.9759 - val_loss: 1.4487 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.49947\n",
      "Epoch 303/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0526 - accuracy: 0.9692 - val_loss: 1.4398 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.49947\n",
      "Epoch 304/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0481 - accuracy: 0.9700 - val_loss: 1.4344 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.49947\n",
      "Epoch 305/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0555 - accuracy: 0.9684 - val_loss: 1.4340 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.49947\n"
     ]
    }
   ],
   "source": [
    "# CNN + LSTM 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding( word_size, max_len ) )\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(5, activation='softmax')) # 다중분류\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile( loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "if os.path.exists( f'/model/CNN+LSTM_0514/num_words_{num_word}/'):\n",
    "    os.mkdir(f'/model/CNN+LSTM_0514/num_words_{num_word}/' )\n",
    "    \n",
    "model_path = 'model/CNN+LSTM_0514/{epoch}-{val_loss}-{val_accuracy}.h5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', \n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',patience=300)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_x, \n",
    "                                                     y_encoded, \n",
    "                                                     test_size=0.3, \n",
    "                                                     stratify=y_encoded)\n",
    "\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit( X_train, y_train, validation_split=0.3,\n",
    "                    epochs=1500,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:30:12.030431Z",
     "start_time": "2021-05-13T16:30:12.026441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 43)          645000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 64)          13824     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 55)                26400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 280       \n",
      "=================================================================\n",
      "Total params: 685,504\n",
      "Trainable params: 685,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN+LSTM 모델\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:30:12.219925Z",
     "start_time": "2021-05-13T16:30:12.031428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트셋으로 실험 결과의 오차값을 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋으로 측정한 정확도의 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# x값을 지정하고 그래프로 확인\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.title('[Figure] CNN+LSTM 모델의 결과')\n",
    "plt.plot( x_len, y_vloss, 'o', c='red', markersize=3, label='TestSet Loss')\n",
    "plt.plot(x_len, y_acc, 'o', c='blue', markersize=3, label='TrainSet Acuraccy')\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.savefig(f'CNN_LSTM_num_words_{num_word}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:30:12.685207Z",
     "start_time": "2021-05-13T16:30:12.220922Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 3ms/step - loss: 0.4673 - accuracy: 0.9034\n",
      "0.9033641219139099\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 1.2009 - accuracy: 0.7792\n",
      "0.7792307734489441\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 정확도\n",
    "print(model.evaluate(X_train,y_train)[1])\n",
    "\n",
    "# 테스트 데이터 정확도\n",
    "print(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 저장한 모델 불러오기\n",
    "- 1. 테스트 데이터의 정확도가 가장 높고( 0.74) val_loss가 가장 낮은 LSTM 모델 사용\n",
    "- 2. val_loss가 가장 낮은(0.54) DNN도 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:33:59.373490Z",
     "start_time": "2021-05-13T16:33:59.367706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22-0.3736964166164398-0.8499278426170349.h5', '다운로드 (1).png']\n",
      "['16-0.4358274042606354-0.8326118588447571.h5', 'DNN_num_words_15000.png']\n",
      "['17-0.46253859996795654-0.822510838508606.h5', '다운로드.png']\n",
      "['16-0.38639506697654724-0.8585858345031738.h5', '다운로드.png']\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('model/DNN_0514/'):\n",
    "    print(os.listdir('model/DNN_0514/' + i)) # 오차가 가장 낮고 정확도가 높은 모델 사용 -> num_words_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:34:26.477336Z",
     "start_time": "2021-05-13T16:34:26.471353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7-0.44331157207489014-0.8670329451560974.h5', '다운로드.png']\n",
      "['5-0.4373718798160553-0.8483516573905945.h5', 'LSTM_num_words_15000.png']\n",
      "['7-0.44055092334747314-0.8472527265548706.h5', 'LSTM_num_words_20000.png']\n",
      "['5-0.43961015343666077-0.8670329451560974.h5', '다운로드 (1).png']\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('model/LSTM_0514/'):\n",
    "    print(os.listdir('model/LSTM_0514/' + i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:34:36.581638Z",
     "start_time": "2021-05-13T16:34:36.575654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4-0.42932772636413574-0.8615384697914124.h5', '다운로드 (1).png']\n",
      "['5-0.4994731843471527-0.8164834976196289.h5', 'CNN_LSTM_num_words_15000.png']\n",
      "['4-0.46037590503692627-0.8406593203544617.h5', 'CNN_LSTM_num_words_20000.png']\n",
      "['5-0.4273459315299988-0.8593406677246094.h5', 'CNN_LSTM_num_words_5000.png']\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('model/CNN+LSTM_0514/'):\n",
    "    print(os.listdir('model/CNN+LSTM_0514/' + i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:36:52.809549Z",
     "start_time": "2021-05-13T16:36:52.753694Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = load_model('./model/DNN_0514/num_words_5000/16-0.38639506697654724-0.8585858345031738.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:30.061030Z",
     "start_time": "2021-05-13T16:37:30.036108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  904, 1704, 4659],\n",
       "       [   0,    0,    0, ..., 1356,  603,  817],\n",
       "       [   0,    0,    0, ..., 1356,  603,  817],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  166, 2258,  745],\n",
       "       [   0,    0,    0, ...,  923, 2640, 1228],\n",
       "       [   0,    0,    0, ...,   80, 2441, 4560]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x = pad_sequences(X, max_len)\n",
    "padded_x # 배열의 길이가 맞춰짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 학습하지 않은 데이터로 후보자 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:48.948265Z",
     "start_time": "2021-05-13T16:37:48.899396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139776"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하지 않은 데이터의 인덱스 번호\n",
    "null_idx = df2[df2['candidate'].isnull()].index\n",
    "\n",
    "# 예측 대상인 텍스트 데이터 분리\n",
    "docs2 = list(df2.iloc[null_idx]['title_comment'])\n",
    "len(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:55.697758Z",
     "start_time": "2021-05-13T16:37:53.084429Z"
    }
   },
   "outputs": [],
   "source": [
    "# 앞서 만든 토큰의 인덱스로만 채워진 새로운 배열 생성\n",
    "X2 = token.texts_to_sequences(docs2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:37:58.538888Z",
     "start_time": "2021-05-13T16:37:58.535680Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[576, 83, 108, 15, 1356, 603, 817, 1522, 210, 3692]\n",
      "재보궐  부산 김영춘  박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애국하는 마음으로 물러서라\n"
     ]
    }
   ],
   "source": [
    "print(X2[0])\n",
    "print(docs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:01.420143Z",
     "start_time": "2021-05-13T16:38:01.409203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "논란\n",
      "이런\n",
      "참\n",
      "그렇게\n",
      "코로나\n",
      "근데\n",
      "전형적인\n",
      "온\n",
      "김태년\n",
      "야권은\n"
     ]
    }
   ],
   "source": [
    "# 기존 토큰의 인덱스로 채워진 배열의 값 확인\n",
    "for key, val  in  token.word_index.items():\n",
    "    if val in [785, 68, 70, 36, 1659, 1409, 1006, 959, 145, 1770]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:27.358028Z",
     "start_time": "2021-05-13T16:38:26.897490Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139776, 33)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩\n",
    "padded_x2 = pad_sequences(X2, max_len)\n",
    "padded_x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:33.432498Z",
     "start_time": "2021-05-13T16:38:30.125580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측하기\n",
    "y_pred_label = np.argmax(best_model.predict(padded_x2), axis = 1)\n",
    "y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:36.188085Z",
     "start_time": "2021-05-13T16:38:36.180097Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['박영선', '박영선', '박영선', ..., '기타', '박영선', '기타'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 디코딩\n",
    "y_pred = encoder.inverse_transform(y_pred_label)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:38.951671Z",
     "start_time": "2021-05-13T16:38:38.946684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'박영선'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:41.843758Z",
     "start_time": "2021-05-13T16:38:41.839769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'재보궐  부산 김영춘  박형준 서울도 양자 대결 참 답없네 학생들 밥주가 싫다고 땡깡부리다 뛰쳐나간 놈을 뽑기도 그렇도'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:44.700559Z",
     "start_time": "2021-05-13T16:38:44.685594Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "기타     67289\n",
       "박영선    47658\n",
       "오세훈    23815\n",
       "박형준      952\n",
       "김영춘       62\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과값 확인\n",
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:38:48.671039Z",
     "start_time": "2021-05-13T16:38:48.660069Z"
    }
   },
   "outputs": [],
   "source": [
    "# 예측한 후보값 넣기\n",
    "df2['area_candidate'][null_idx] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:39:42.001597Z",
     "start_time": "2021-05-13T16:39:41.986637Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_eval</th>\n",
       "      <th>party</th>\n",
       "      <th>party_eval</th>\n",
       "      <th>title_comment</th>\n",
       "      <th>area_candidate</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애국하는 마...</td>\n",
       "      <td>박영선</td>\n",
       "      <td>조선일보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 박영선은 정동영이 얻은 프로선에 머...</td>\n",
       "      <td>박영선</td>\n",
       "      <td>조선일보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 빵선이가서울시장되면서울은공산국가수도...</td>\n",
       "      <td>박영선</td>\n",
       "      <td>조선일보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 서울시장후보더듬당박빵선이는절대로서울...</td>\n",
       "      <td>박영선</td>\n",
       "      <td>조선일보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 부산은오거돈선거이고 오거돈치부선거아...</td>\n",
       "      <td>박영선</td>\n",
       "      <td>조선일보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144134</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 국민...</td>\n",
       "      <td>기타</td>\n",
       "      <td>경향신문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144135</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 굳이...</td>\n",
       "      <td>박영선</td>\n",
       "      <td>경향신문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144136</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 도랏...</td>\n",
       "      <td>기타</td>\n",
       "      <td>경향신문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144137</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 이걸...</td>\n",
       "      <td>박영선</td>\n",
       "      <td>경향신문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144138</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 우리...</td>\n",
       "      <td>기타</td>\n",
       "      <td>경향신문</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144139 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        area  candidate  candidate_eval  party  party_eval  \\\n",
       "0        NaN        NaN             NaN    NaN         NaN   \n",
       "1        1.0        1.0             0.0    NaN         NaN   \n",
       "2        1.0        1.0             0.0    NaN         NaN   \n",
       "3        1.0        1.0             0.0    1.0         0.0   \n",
       "4        2.0        NaN             NaN    1.0         0.0   \n",
       "...      ...        ...             ...    ...         ...   \n",
       "144134   NaN        NaN             NaN    NaN         NaN   \n",
       "144135   NaN        NaN             NaN    NaN         NaN   \n",
       "144136   NaN        NaN             NaN    NaN         NaN   \n",
       "144137   NaN        NaN             NaN    NaN         NaN   \n",
       "144138   NaN        NaN             NaN    NaN         NaN   \n",
       "\n",
       "                                            title_comment area_candidate  \\\n",
       "0       재보궐  부산 김영춘  박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애국하는 마...            박영선   \n",
       "1       재보궐  부산 김영춘  박형준 서울도 양자 대결 박영선은 정동영이 얻은 프로선에 머...            박영선   \n",
       "2       재보궐  부산 김영춘  박형준 서울도 양자 대결 빵선이가서울시장되면서울은공산국가수도...            박영선   \n",
       "3       재보궐  부산 김영춘  박형준 서울도 양자 대결 서울시장후보더듬당박빵선이는절대로서울...            박영선   \n",
       "4       재보궐  부산 김영춘  박형준 서울도 양자 대결 부산은오거돈선거이고 오거돈치부선거아...            박영선   \n",
       "...                                                   ...            ...   \n",
       "144134  논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 국민...             기타   \n",
       "144135  논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 굳이...            박영선   \n",
       "144136  논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 도랏...             기타   \n",
       "144137  논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 이걸...            박영선   \n",
       "144138  논설위원의 단도직입한일해저터널 경제성 없지만동북아 경제권 차원서 장기 검토해야 우리...             기타   \n",
       "\n",
       "       source  \n",
       "0        조선일보  \n",
       "1        조선일보  \n",
       "2        조선일보  \n",
       "3        조선일보  \n",
       "4        조선일보  \n",
       "...       ...  \n",
       "144134   경향신문  \n",
       "144135   경향신문  \n",
       "144136   경향신문  \n",
       "144137   경향신문  \n",
       "144138   경향신문  \n",
       "\n",
       "[144139 rows x 8 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df2['source'] = df['출처']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T16:40:09.391891Z",
     "start_time": "2021-05-13T16:40:08.635729Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.to_csv('./data/prediction_byModel2/data_byModel2_0514_0140.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
