{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import time\n",
    "import platform\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한겨레 뉴스 링크 크롤링 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청 함수\n",
    "def getSource(site) :\n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb Kit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # 요청한다.\n",
    "    response = requests.get(site, headers=header_info)\n",
    "    \n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지에 있는 다음 뉴스 링크 수집 함수\n",
    "def getNewsLink(soup):\n",
    "\n",
    "    link_list = []\n",
    "\n",
    "    # li 태그 가져오기\n",
    "    a1 = soup.select('#clusterResultUL > li')\n",
    "    # print(len(a1))\n",
    "    \n",
    "    for a2 in a1:\n",
    "        \n",
    "        # div 태그 가져오기\n",
    "        a3 = a2.select_one('div.wrap_cont > div > div > a')\n",
    "        # print(a3)\n",
    "        \n",
    "        # 기사링크 \n",
    "        data1 = a3.attrs['href']\n",
    "        # print(data1)\n",
    "        \n",
    "        # 기사제목\n",
    "        data2 = a3.text.strip()\n",
    "        # print(data2)\n",
    "    \n",
    "        # span 태그 가져오기\n",
    "        a4 = a2.select_one('div.wrap_cont > div > span.f_nb.date')\n",
    "        a5 = a4.text.strip().split('|')\n",
    "        \n",
    "        # 날짜\n",
    "        data3 = a5[0]\n",
    "        \n",
    "        # 언론사\n",
    "        data4 = a5[1]\n",
    "        \n",
    "        # print(data1, data2, data3, data4)\n",
    "        \n",
    "        # 기사 링크 리스트에 저장\n",
    "        link_list.append(data1)\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df1 = pd.DataFrame(link_list)\n",
    "    display(df1)\n",
    "    \n",
    "    FILENAME = 'hani_link.csv'\n",
    "    if os.path.exists(FILENAME) == False:\n",
    "        # 파일이 없을 경우\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False)\n",
    "    else:\n",
    "        # mode='a' : 기존 것 뒤에다 붙여줌\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False, header=False, mode='a')\n",
    "    \n",
    "        \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 페이지 존재 여부 확인하는 함수\n",
    "def getNextPage(site) :\n",
    "    \n",
    "    # url에서 p= 값 들고오기\n",
    "    p = site.split('&')[-1].split('=')[-1]\n",
    "    \n",
    "    # p값에 1 더해서 다음 페이지 url 만들기\n",
    "    nextPage = site[:-len(p)] + str(int(p)+1)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 soup 가져오기\n",
    "    soup1 = getSource(site)\n",
    "    soup2 = getSource(nextPage)\n",
    "    # print(soup1)\n",
    "    # print(soup2)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 첫번째 a 태그에서 링크 가져오기\n",
    "    a1 = soup1.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    a2 = soup2.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    print(a1)\n",
    "    print(a2)\n",
    "    \n",
    "    # 두 링크가 같지 않으면 다음 페이지가 있다고 간주, 다음 페이지 return \n",
    "    if a1 != a2 :\n",
    "        return True\n",
    "    # 같으면 다음 페이지 없다고 간주, False return \n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한겨레 url을 담은 csv 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 다음 뉴스 - 한겨레 : 45 페이지 수집 중\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://v.media.daum.net/v/20210316200604054?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://v.media.daum.net/v/20210315205607403?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://v.media.daum.net/v/20210314101604196?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://v.media.daum.net/v/20210307135609306?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://v.media.daum.net/v/20210303165614030?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://v.media.daum.net/v/20210304162601513?f=o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0\n",
       "0  http://v.media.daum.net/v/20210316200604054?f=o\n",
       "1  http://v.media.daum.net/v/20210315205607403?f=o\n",
       "2  http://v.media.daum.net/v/20210314101604196?f=o\n",
       "3  http://v.media.daum.net/v/20210307135609306?f=o\n",
       "4  http://v.media.daum.net/v/20210303165614030?f=o\n",
       "5  http://v.media.daum.net/v/20210304162601513?f=o"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://v.media.daum.net/v/20210316200604054?f=o\n",
      "http://v.media.daum.net/v/20210316200604054?f=o\n",
      "수집완료\n"
     ]
    }
   ],
   "source": [
    "KEYWORD = urllib.parse.quote('보궐선거')\n",
    "COLOPHON = '한겨레'\n",
    "\n",
    "dayStart = '20210301000001'\n",
    "dayEnd = '20210406120000'\n",
    "\n",
    "page = 1\n",
    "URL = 'https://search.daum.net/search?w=news&enc=utf8&cluster=y&cluster_page=1&'\n",
    "cp_dict = {'조선일보' : '16d4PV266g2j-N3GYq',\n",
    "           '중앙일보' : '16Elf9uX5H6T5xXvQV',\n",
    "           '동아일보' : '16bOiOx4gG2S18EPLj',\n",
    "           'JTBC'     : '16yZfDfR_rGcw5F-P0',\n",
    "           '경향신문' : '16akMkKFDu6n8GTzZr',\n",
    "           '한겨레' : '16nzyJHdH5ORpabfqG'}\n",
    "cpName = urllib.parse.quote(COLOPHON)\n",
    "cp = cp_dict[COLOPHON]\n",
    "\n",
    "while True :\n",
    "    time.sleep(1)\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "        \n",
    "    site = f'{URL}q={KEYWORD}&cpname={cpName}&cp={cp}&period=6m&sd={dayStart}&ed={dayEnd}&DA=PGD&p={page}'\n",
    "\n",
    "    print(f' 다음 뉴스 - {COLOPHON} : {page} 페이지 수집 중' )\n",
    "    \n",
    "    soup = getSource(site)\n",
    "    getNewsLink(soup) \n",
    "    chk = getNextPage(site)\n",
    "\n",
    "    if chk != False:\n",
    "        page = page + 1\n",
    "    else: \n",
    "        print('수집완료')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한겨레 댓글 크롤링 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글 iframe 가져오는 함수\n",
    "def getCommentIframe():\n",
    "    # 댓글 iframe이 없을 경우 처리가가 복잡해지니 iframe이 나올 때 까지 반복한다.\n",
    "    chk = True\n",
    "\n",
    "    while chk :\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 원하는 기사를 요청한다.\n",
    "\n",
    "        # 가장 아래로 스크롤 한다.\n",
    "        # 스크롤을 해야 댓글이 나옴\n",
    "        driver.execute_script(\"window.scrollTo(0, 50000)\") \n",
    "\n",
    "        # iframe들을 모두 가져온다.\n",
    "        iframe_list = driver.find_elements_by_tag_name('iframe')\n",
    "\n",
    "        # iframe의 수 만큼 반복한다.\n",
    "        for a1 in iframe_list :\n",
    "            # iframe의 title 속성값을 가져온다.\n",
    "            title1 = a1.get_attribute('title')\n",
    "\n",
    "            # title 속성값이 livere-comment 라면 src 속성을 추출한다.\n",
    "            if title1 == 'livere-comment' :\n",
    "                src1 = a1.get_attribute('src')\n",
    "                # print(src1)\n",
    "                return src1\n",
    "                # 댓글 iframe을 발견하였으므로 while이 종료될 수 있게 한다.\n",
    "                chk = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더보기 계속 클릭하기\n",
    "def clickMore(driver):\n",
    "            \n",
    "   # 댓글 더보기 클릭\n",
    "    while True :\n",
    "        try :\n",
    "            time.sleep(1)\n",
    "            driver.find_element_by_class_name('more-btn').click()\n",
    "            # print('클릭')\n",
    "\n",
    "        except :\n",
    "            # print('클릭 끝')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComment(driver, data1, data2) :\n",
    "    \n",
    "    # 데이터를 담을 딕셔너리\n",
    "    comment_dict = {\n",
    "        '제목' : [],\n",
    "        '날짜' : [],\n",
    "        '작성일' : [],\n",
    "        '댓글' : []\n",
    "    }\n",
    "    \n",
    "    a1 = driver.find_elements_by_tag_name('#list > div.reply-wrapper')\n",
    "    # 댓글이 없으면\n",
    "    if not a1:\n",
    "        return False\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for a2 in a1:\n",
    "            a3 = a2.find_elements_by_tag_name('div.reply-top > div > ul')[0]\n",
    "            \n",
    "            # 사용자 id \n",
    "            a4 = a3.find_elements_by_tag_name('li.writer-name')[0].text\n",
    "            if a4 == '삭제된 댓글입니다.':\n",
    "                print(a4)\n",
    "                continue\n",
    "\n",
    "            # 작성일\n",
    "            data3 = a3.find_elements_by_tag_name('li.reply-history > div > span')[0].text\n",
    "            print(data3)\n",
    "\n",
    "            # 댓글 & 답글 태그\n",
    "            a5 = a2.find_elements_by_tag_name('div.reply-bottom')[0]\n",
    "            \n",
    "            # 답글 유무 확인\n",
    "            reply = a5.find_elements_by_tag_name('div.reply-content-wrapper > div.reply-btn-group > div.left > button')[0].text\n",
    "            reply_css_selector = 'div.child-reply'\n",
    "            # print('응답 시작 : ' + reply)\n",
    "            \n",
    "            if reply == '댓글보기' or reply == '':\n",
    "                continue\n",
    "                        \n",
    "\n",
    "            # 댓글 내용\n",
    "            data4 = a5.find_elements_by_tag_name('div.reply-content-wrapper > div.reply-content > p')[0].text\n",
    "            \n",
    "            comment_dict['제목'].append(data1)\n",
    "            comment_dict['날짜'].append(data2)\n",
    "            comment_dict['작성일'].append(data3)\n",
    "            comment_dict['댓글'].append(data4)\n",
    "                \n",
    "        df = pd.DataFrame(comment_dict)\n",
    "        display(df)\n",
    "        \n",
    "        FILENAME = 'hani_comment.csv'\n",
    "        if os.path.exists(FILENAME) == False:\n",
    "            # 파일이 없을 경우\n",
    "            df.to_csv(FILENAME, encoding='utf-8-sig', index=False)\n",
    "        else:\n",
    "            # mode='a' : 기존 것 뒤에다 붙여줌\n",
    "            df.to_csv(FILENAME, encoding='utf-8-sig', index=False, header=False, mode='a')\n",
    "\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 제목 & 날짜 수집\n",
    "def getTitleDate(driver):\n",
    "    # 기사 제목\n",
    "    \n",
    "    title = driver.find_element_by_css_selector('#article_view_headline > h4 > span').text\n",
    "    \n",
    "    # 기사 날짜\n",
    "    date = driver.find_element_by_css_selector('#article_view_headline > p.date-time > span:nth-child(1)').text\n",
    "    # print(date[4:])\n",
    "    \n",
    "    return title, date[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 데이터를 csv 파일로 저장하는 함수\n",
    "def saveData(driver):\n",
    "    \n",
    "    # 기사의 제목과 날짜를 가져온다\n",
    "    data1, data2 = getTitleDate(driver)\n",
    "    # 기사의 댓글 iframe을 가져온다\n",
    "    src1 = getCommentIframe()\n",
    "\n",
    "    # 댓글 페이지를 요청한다.\n",
    "    driver.get(url=src1)\n",
    "    # 더보기 클릭\n",
    "    clickMore(driver)\n",
    "    # 데이터 들고오기\n",
    "    chk = getComment(driver, data1, data2)\n",
    "    \n",
    "    return chk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한겨레 댓글 크롤링 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300번째 기사 댓글 수집 중\n"
     ]
    }
   ],
   "source": [
    "link_df = pd.read_csv('hani_link.csv')\n",
    "num_link = link_df.shape[0]\n",
    "# print(link_df.loc[0])\n",
    "\n",
    "# 웹 드라이버\n",
    "driver = wd.Chrome('./chromedriver.exe')\n",
    "idx = 0\n",
    "\n",
    "while True :\n",
    "    \n",
    "    driver.implicitly_wait(20)\n",
    "       \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    driver.get(link_df.loc[idx].values[0])\n",
    "\n",
    "    print(f'{idx}번째 기사 댓글 수집 중')\n",
    "\n",
    "    chk = saveData(driver)\n",
    "    if chk == 1:\n",
    "        print(f'{idx}번째 기사 댓글 정상 수집 완료')\n",
    "    else:\n",
    "        print(f'{idx}번째 기사 댓글 없음, 다음 기사로')\n",
    "    \n",
    "    idx = idx + 1\n",
    "    if idx == num_link :\n",
    "        break\n",
    "\n",
    "         \n",
    "print('수집완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
